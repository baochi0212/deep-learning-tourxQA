{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(32, 8)\n",
    "b = torch.zeros([32, 8], dtype=torch.float)\n",
    "b[0, 0] = 1\n",
    "loss = nn.BCEWithLogitsLoss(reduction='none')(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.where(b != 0, loss, torch.tensor([0.]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.where(b != 0, loss, loss*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 5  # number of tags is 5\n",
    "model = CRF(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "emissions = torch.randn(seq_length, batch_size, num_tags) #n_seq x batch x logits\n",
    "tags = torch.tensor([\n",
    "  [0, 1], [2, 4], [3, 1]\n",
    "], dtype=torch.long)  #nseq x batch\n",
    "loss = model(emissions, tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(30, 32, 142)\n",
    "b = torch.zeros(30, 32, dtype=torch.long)\n",
    "model = CRF(a.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4717.1206, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "name = ['train', 'test', 'dev']\n",
    "for i in name:\n",
    "    for label in open(\"/home/xps/educate/code/hust/XQA/docs/src/JointIDSF/PhoATIS/word-level\" + f'/{i}/label'):\n",
    "        if label.strip() not in list:\n",
    "            list.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_train.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_flight_time', 'atis_airfare', 'atis_aircraft',\n",
       "        'atis_ground_service', 'atis_airline', 'atis_abbreviation',\n",
       "        'atis_quantity'], dtype=object),\n",
       " 4834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'vietanh': 'ngu'}\n",
    "a.update({'vie anh': 'ocho'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_airfare', 'atis_ground_service',\n",
       "        'atis_airline', 'atis_flight_time', 'atis_quantity',\n",
       "        'atis_abbreviation', 'atis_aircraft'], dtype=object),\n",
       " 800)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_test.csv\", header=None)\n",
    "test[0].unique(), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tôi muốn một chuyến bay khứ_hồi từ đồng_hới đến côn_đảo khởi_hành vào thứ tư ngày 16 tháng 6 đến côn_đảo vào buổi tối khoảng 7 giờ và trở về đồng_hới vào buổi tối ngày hôm sau khoảng 7 giờ hãng hàng_không nào có chuyến bay phù_hợp với tuyến đường đó\n"
     ]
    }
   ],
   "source": [
    "lengths = {}\n",
    "with open(\"/home/xps/educate/code/hust/XQA/data/raw/PhoATIS/word-level/train/seq.in\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line.strip().split(' ')) not in lengths.keys():\n",
    "            lengths[len(line.strip().split(' '))] = 1\n",
    "        else:\n",
    "            lengths[len(line.strip().split(' '))] += 1 \n",
    "        if len(line.strip().split(' ')) == 50:\n",
    "            print(line.strip()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47,\n",
       " 33,\n",
       " 50,\n",
       " 37,\n",
       " 1,\n",
       " 38,\n",
       " 2,\n",
       " 36,\n",
       " 34,\n",
       " 31,\n",
       " 30,\n",
       " 29,\n",
       " 28,\n",
       " 32,\n",
       " 25,\n",
       " 27,\n",
       " 26,\n",
       " 3,\n",
       " 24,\n",
       " 23,\n",
       " 4,\n",
       " 22,\n",
       " 21,\n",
       " 5,\n",
       " 6,\n",
       " 20,\n",
       " 19,\n",
       " 7,\n",
       " 17,\n",
       " 18,\n",
       " 8,\n",
       " 16,\n",
       " 9,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 11]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths, key = lambda d: lengths[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoctro'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename('/hoa/hoctro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "os.environ['dir'] = \"/home/xps/educate/code/hust/XQA/data\"\n",
    "from utils.preprocess import string2list, get_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@đương nhiên \"công nhận\" chủ quyền của Trung Quốc đối với các quần đảo trên biển Đông\n"
     ]
    }
   ],
   "source": [
    "for i in df['text']:\n",
    "    if i[1] == '\"':\n",
    "        print(i)\n",
    "    else:\n",
    "        if '\"' in i:\n",
    "            print(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'các vùng người Đức',\n",
       " 'các vùng người Đức',\n",
       " '\"các vùng người Đức\"',\n",
       " 'các vùng người Đức']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.split('@@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for i in range(len(df)):\n",
    "    item = df.iloc[i]\n",
    "    start = string2list(item['start'], type='int')\n",
    "    text = string2list(item['text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wefwefwefwfe', 'dfsdfsdf']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"wefwefwefwfe@@@dfsdfsdf\"\n",
    "a.split(\"@@@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4336,  0.1250, -0.4940,  ..., -0.0956,  0.3486, -0.1417],\n",
       "         [-0.1834,  0.1658, -0.2279,  ..., -0.2353, -0.2254, -0.2486],\n",
       "         [-0.1474,  0.2150,  0.1409,  ..., -0.2747, -0.2477,  0.1704],\n",
       "         ...,\n",
       "         [-0.6622,  0.4771, -0.2023,  ..., -0.4579,  0.0228, -0.0134],\n",
       "         [-0.6622,  0.4771, -0.2023,  ..., -0.4579,  0.0228, -0.0134],\n",
       "         [-0.6622,  0.4771, -0.2023,  ..., -0.4579,  0.0228, -0.0134]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 1.9058e-01,  1.4902e-01, -1.0911e-01, -1.3144e-01, -1.1396e-01,\n",
       "          3.8867e-02,  1.8498e-01, -4.0306e-02,  1.3853e-01, -2.5466e-01,\n",
       "         -3.0302e-01, -1.8956e-01, -1.9188e-01, -1.0153e-01,  3.8096e-01,\n",
       "          1.0175e-01,  1.6042e-01,  1.6552e-01, -3.2850e-01, -2.4631e-01,\n",
       "          4.6491e-02, -4.0419e-01,  2.8747e-02, -1.2116e-01,  1.7839e-01,\n",
       "         -3.1060e-01, -1.5783e-01, -2.6581e-02, -2.2371e-01,  5.1163e-02,\n",
       "         -2.0195e-01, -8.9764e-02,  1.4329e-02,  1.9347e-01,  5.3215e-02,\n",
       "         -3.8200e-02, -3.5856e-01,  1.5147e-01, -7.7431e-02,  3.5235e-01,\n",
       "         -2.1355e-01,  4.7890e-01, -1.1631e-01, -3.2858e-01,  1.0641e-01,\n",
       "         -5.3863e-02,  2.2223e-01,  5.5828e-01, -1.5367e-01, -1.4258e-01,\n",
       "          2.8534e-01, -2.2133e-01,  1.5641e-02, -2.3708e-02,  3.1971e-01,\n",
       "         -2.9451e-02,  1.5348e-01,  6.6179e-01, -8.2429e-02,  2.3516e-01,\n",
       "          4.0823e-01,  4.0262e-01, -2.8304e-01, -9.9235e-02, -6.1153e-02,\n",
       "         -7.7653e-02,  1.6547e-01, -2.7590e-03, -1.8735e-01,  3.1917e-01,\n",
       "         -8.2408e-02, -3.6607e-01,  4.3652e-02,  2.8186e-02, -3.3467e-02,\n",
       "         -2.0613e-01,  5.7565e-03, -1.5580e-01, -3.4371e-01,  2.8333e-01,\n",
       "          7.4514e-02,  4.3427e-01, -2.1815e-01,  2.2473e-01, -5.0654e-02,\n",
       "         -5.3119e-02, -2.2357e-01, -2.1891e-01, -1.0161e-01,  1.6337e-01,\n",
       "          1.9378e-01, -4.6669e-02,  2.4797e-02, -1.7626e-01, -6.9454e-02,\n",
       "         -1.5423e-01,  1.7691e-01, -1.2056e-01, -8.1620e-02, -1.1731e-01,\n",
       "          1.0273e-01,  3.9280e-02, -3.6982e-01, -5.6611e-01,  1.1784e-01,\n",
       "         -3.6273e-01, -1.2319e-01, -1.5489e-02,  1.8809e-01, -3.3074e-01,\n",
       "         -3.0016e-01, -3.1016e-01,  4.8339e-01,  2.6954e-01, -1.9899e-01,\n",
       "         -1.6914e-01,  6.5849e-02, -3.9637e-02, -3.5580e-01, -9.6749e-03,\n",
       "          4.7359e-02,  7.6387e-02,  7.0539e-02,  2.2982e-01, -4.7081e-01,\n",
       "          2.9007e-01, -2.5992e-01, -9.5011e-02, -2.7464e-01,  1.3242e-02,\n",
       "         -1.6022e-01,  2.0788e-01,  3.0163e-01, -2.1081e-01,  8.4137e-02,\n",
       "         -5.2971e-03, -1.7641e-02,  1.1794e-01,  2.1087e-01,  1.9192e-01,\n",
       "          1.5551e-01,  1.4479e-01, -2.0798e-01, -2.1058e-01,  7.3023e-02,\n",
       "          1.7605e-01,  2.1151e-01, -1.6424e-01,  5.1123e-02, -4.0868e-01,\n",
       "         -3.9994e-01,  1.3425e-01, -9.8629e-02,  3.6542e-01, -4.7572e-01,\n",
       "         -1.9573e-01,  1.4850e-01,  3.7792e-02,  1.5751e-01,  9.6929e-02,\n",
       "         -3.5828e-01,  1.5186e-01, -1.3867e-01, -4.3892e-01, -1.1013e-01,\n",
       "          2.4426e-01,  7.4330e-02, -9.4349e-02, -2.2867e-01,  2.9908e-03,\n",
       "         -8.6036e-02, -7.0930e-02,  1.0756e-02,  3.1045e-01,  2.4414e-01,\n",
       "         -1.2307e-01,  3.0534e-02, -1.1671e-01, -9.4995e-02,  3.3828e-01,\n",
       "          1.7240e-01, -2.5056e-01,  2.9888e-02,  3.4369e-01, -6.2250e-02,\n",
       "         -1.5243e-01, -1.2915e-01, -4.4313e-02,  1.2603e-01,  5.5323e-02,\n",
       "          2.5659e-03,  6.2546e-02,  1.8701e-01, -3.7110e-02, -2.5653e-01,\n",
       "         -2.2551e-01,  1.8432e-01,  2.9775e-01,  1.5100e-01, -9.1651e-02,\n",
       "         -1.4398e-01,  2.4649e-01, -1.0486e-01,  2.9752e-02,  5.2631e-02,\n",
       "         -1.5571e-01, -1.8039e-01,  2.5388e-02,  2.2808e-02,  2.2608e-01,\n",
       "         -1.5528e-01,  1.3554e-01,  2.1417e-01, -4.8313e-02, -8.0851e-02,\n",
       "          3.2343e-01,  1.5289e-01, -3.7597e-01,  7.1807e-02,  1.4327e-01,\n",
       "         -1.5247e-01, -3.4469e-01,  1.9030e-01,  1.4508e-01,  1.3998e-01,\n",
       "         -6.4734e-02, -1.2411e-01, -1.1953e-01,  1.0162e-01, -1.2839e-02,\n",
       "         -4.0359e-01, -3.6006e-02, -3.1560e-01,  5.4818e-02, -1.5839e-01,\n",
       "          5.4498e-02,  1.6089e-01, -3.9649e-01, -2.1429e-02, -2.3288e-03,\n",
       "         -7.7462e-02,  4.8960e-02, -1.7666e-01, -3.6162e-01, -1.4981e-02,\n",
       "         -2.5517e-01, -1.2056e-02,  2.6797e-01, -1.9184e-01, -1.2389e-01,\n",
       "          4.1644e-01,  2.4287e-01,  4.5578e-02,  1.5466e-01,  1.0836e-01,\n",
       "         -3.1931e-01, -6.9219e-02,  4.7681e-01,  2.6940e-01,  8.1219e-02,\n",
       "         -1.0659e-01, -2.7798e-01,  5.5324e-03,  6.7609e-03, -7.6695e-02,\n",
       "         -2.2070e-01,  2.5262e-01,  4.9075e-02, -2.2752e-01,  1.3912e-01,\n",
       "          2.6812e-01, -2.2937e-02, -1.5795e-01,  1.5219e-01, -6.0643e-02,\n",
       "          2.1142e-01, -2.2350e-02, -2.7552e-01, -2.2869e-01, -8.6275e-03,\n",
       "          2.2725e-01, -2.7472e-01, -2.3098e-01,  5.4804e-02,  2.7043e-01,\n",
       "          4.5926e-02, -1.5103e-01,  4.1524e-02, -2.2710e-01, -2.5230e-01,\n",
       "         -1.4595e-01,  9.9010e-02,  9.6859e-02, -1.5835e-01,  4.1764e-02,\n",
       "         -3.8995e-02,  1.3638e-02,  2.0151e-02, -1.7425e-01,  2.4338e-01,\n",
       "         -3.3755e-01, -2.8863e-01, -4.0808e-02, -4.3739e-03,  1.1302e-01,\n",
       "         -1.3089e-01,  1.8627e-01,  1.5247e-01,  5.6463e-02, -1.1758e-01,\n",
       "         -4.6220e-02,  6.8172e-02,  2.8009e-01, -3.4045e-01, -1.7777e-02,\n",
       "          1.1311e-01,  2.7737e-01,  3.1035e-01, -2.7628e-01, -2.8957e-01,\n",
       "         -1.1605e-01,  7.3423e-02, -3.4837e-01,  2.9188e-01, -1.3944e-01,\n",
       "          1.1419e-01,  1.9988e-01,  1.2075e-01, -3.7846e-01,  3.1997e-01,\n",
       "         -7.5335e-02,  2.1983e-01,  6.7943e-02, -2.5343e-01,  1.7029e-01,\n",
       "         -3.4123e-01,  4.3312e-02,  1.4708e-01,  2.2826e-03, -3.3000e-01,\n",
       "         -2.2017e-01,  6.2131e-02,  1.0746e-01, -2.9766e-01, -3.5642e-01,\n",
       "          5.0677e-01, -2.5591e-01,  4.0257e-01, -8.0152e-02, -1.0503e-01,\n",
       "          1.1872e-01, -2.2234e-01,  1.2005e-01,  9.5202e-02,  1.6752e-03,\n",
       "         -2.8105e-01, -2.9978e-01,  1.2163e-01,  6.0614e-02, -6.1732e-02,\n",
       "          1.5557e-01,  2.4160e-01,  1.5096e-01,  2.1495e-01,  6.6541e-02,\n",
       "          5.4210e-02, -6.8903e-02,  9.8629e-02, -3.5670e-02, -9.4456e-02,\n",
       "          1.9399e-01, -2.2017e-01,  8.2494e-02, -2.3380e-01,  1.2296e-01,\n",
       "          1.3231e-01, -2.6431e-01,  8.7038e-02,  1.8056e-01,  2.5719e-01,\n",
       "         -1.4867e-02, -1.0836e-02, -2.4237e-01, -4.1551e-01, -3.5126e-01,\n",
       "         -3.6545e-01,  1.4043e-01,  1.2299e-01, -1.8568e-02,  2.1601e-01,\n",
       "         -1.9323e-01,  4.2621e-01, -1.5504e-02,  2.2941e-01, -4.1895e-01,\n",
       "         -1.4754e-02, -2.6955e-01,  5.3938e-02, -2.7778e-01, -7.0749e-02,\n",
       "         -6.2675e-02, -5.7527e-02,  2.7059e-02,  3.9494e-02,  3.5793e-01,\n",
       "         -1.7593e-02,  1.9826e-01, -2.4453e-01, -8.3882e-02,  1.2170e-01,\n",
       "          4.2200e-02,  2.6549e-03,  1.1468e-01,  5.5451e-02,  7.4700e-02,\n",
       "          4.0501e-01,  2.6597e-01,  2.8381e-02,  1.7926e-01, -2.5860e-01,\n",
       "         -1.8286e-02, -2.9205e-01,  8.5693e-03, -1.1668e-01, -2.2471e-01,\n",
       "          8.5928e-02,  1.0392e-01,  2.1351e-01,  2.5664e-01,  1.2767e-01,\n",
       "          2.0165e-01,  1.6262e-02, -1.0875e-01,  7.9258e-02,  7.6201e-02,\n",
       "          2.8811e-01,  2.7378e-02,  1.5040e-01,  1.1108e-01,  2.5364e-01,\n",
       "          1.2824e-01,  1.6447e-01,  3.6395e-01,  1.2923e-01,  3.7220e-01,\n",
       "         -1.7330e-01, -4.2625e-01,  9.0193e-02, -6.9642e-02,  2.5188e-01,\n",
       "          2.9028e-01, -2.8482e-01, -1.4156e-01,  1.3735e-01,  2.3307e-01,\n",
       "         -1.9364e-01, -7.8422e-02,  2.7344e-01,  4.4032e-02, -1.0606e-01,\n",
       "         -6.4403e-02,  9.8477e-02,  1.9559e-03, -3.7876e-02,  3.7159e-01,\n",
       "          5.1259e-02, -1.7146e-01, -8.7675e-02,  5.1755e-02, -6.7730e-03,\n",
       "          6.2706e-02, -2.2198e-01, -1.4281e-01,  6.5885e-02,  1.7338e-01,\n",
       "         -1.1717e-04,  2.9056e-02,  1.7321e-01, -9.4133e-02, -1.5391e-01,\n",
       "         -1.4010e-01,  1.8363e-01,  3.5124e-01, -3.0040e-02,  3.3101e-03,\n",
       "         -2.4442e-01,  1.6141e-01,  2.2088e-01,  8.4154e-02, -2.6603e-01,\n",
       "         -2.4661e-01,  3.6398e-01,  3.1480e-02, -2.6758e-01,  2.8404e-03,\n",
       "          2.0085e-01,  1.5911e-01,  2.1768e-01,  6.1156e-02,  1.4656e-01,\n",
       "          1.2361e-01,  4.5047e-01, -2.5061e-01, -8.0942e-03,  1.1367e-01,\n",
       "          8.3205e-02, -2.9995e-01,  7.8186e-02, -6.3539e-02,  4.1243e-01,\n",
       "          1.3948e-02,  2.4314e-01,  2.5932e-01,  2.1549e-01, -1.4058e-01,\n",
       "         -3.0738e-01,  1.4041e-01,  9.5310e-02, -1.6055e-02, -5.3864e-01,\n",
       "         -3.3352e-01,  7.7353e-02,  3.2815e-01,  1.8939e-01,  3.3960e-01,\n",
       "         -1.3007e-02,  1.7809e-01, -1.3611e-01, -7.8969e-02,  1.3709e-02,\n",
       "         -1.1177e-02, -8.2036e-02,  1.0528e-01,  2.2055e-01, -4.1377e-02,\n",
       "          1.9586e-01,  3.3960e-05,  4.2463e-02, -3.9005e-01, -2.1940e-01,\n",
       "         -4.3909e-01, -2.3701e-01, -1.1003e-01,  8.1043e-03,  3.3503e-01,\n",
       "          2.6301e-01,  1.7108e-01,  4.6428e-01, -3.2326e-02,  4.6060e-02,\n",
       "          6.5489e-02, -6.0973e-02, -5.2023e-02,  1.4657e-01,  6.6509e-02,\n",
       "          1.8371e-01,  1.1457e-01, -2.2115e-01, -2.3957e-01, -1.1501e-01,\n",
       "          2.2200e-02,  1.0713e-01,  2.2976e-01, -1.1125e-01, -1.1545e-01,\n",
       "          6.3309e-02, -7.7639e-02, -1.7284e-01, -3.4012e-02, -2.1266e-01,\n",
       "         -2.0717e-01, -1.4815e-01, -1.6359e-03,  4.7379e-02,  4.0429e-02,\n",
       "         -4.0971e-03,  2.3670e-01, -6.7539e-02,  2.5697e-01,  1.0546e-01,\n",
       "          1.5274e-01,  9.5623e-02,  3.8816e-02, -6.1034e-02, -2.4228e-01,\n",
       "          5.8881e-01,  2.7860e-01, -1.4728e-01, -1.1858e-01, -2.5405e-02,\n",
       "          7.4489e-03, -1.7583e-01,  9.2756e-02,  6.7630e-03,  1.0755e-01,\n",
       "          1.0091e-01,  1.9025e-01,  8.2910e-02,  4.0404e-01,  5.2596e-02,\n",
       "         -1.1841e-01,  2.0961e-01, -2.4150e-03, -3.6226e-02,  7.1247e-03,\n",
       "         -1.7898e-01,  2.3278e-01,  3.8767e-01,  1.1250e-01,  2.4652e-01,\n",
       "          2.0778e-01, -2.2707e-01, -2.8547e-01, -2.8044e-01, -1.5976e-01,\n",
       "         -1.2040e-01, -2.5448e-01,  4.7388e-01,  1.3856e-01, -7.3259e-02,\n",
       "          3.0768e-02, -1.3515e-01, -1.2945e-01,  1.6565e-01, -5.3966e-02,\n",
       "          1.9433e-01, -2.6771e-02,  3.9975e-02, -1.6831e-03,  2.0422e-01,\n",
       "         -2.6992e-01, -2.0117e-01,  1.6394e-01,  2.2265e-01, -1.2661e-01,\n",
       "          2.2212e-01,  1.7388e-02, -2.6805e-01,  5.6489e-02,  5.9336e-02,\n",
       "          2.4120e-01, -9.8794e-03, -2.1129e-01, -8.9154e-02, -2.6135e-01,\n",
       "         -2.3170e-02, -1.2254e-01,  3.6159e-01,  9.8520e-02,  1.2001e-01,\n",
       "          5.9533e-02,  3.5101e-02,  9.1410e-02, -5.0239e-03,  2.7561e-02,\n",
       "          7.4379e-02, -7.0163e-02,  8.2993e-02, -1.1427e-01, -2.1241e-01,\n",
       "         -4.8483e-02, -6.5676e-02, -1.5295e-01, -9.4958e-02,  6.0342e-02,\n",
       "          4.1079e-01,  9.0501e-03,  2.6086e-01, -2.9254e-01,  6.9872e-02,\n",
       "          7.5301e-02,  9.1346e-02,  1.4755e-01, -1.7439e-01,  2.5941e-01,\n",
       "          1.1609e-01, -3.4624e-01, -1.7566e-01, -4.7018e-01, -4.2699e-02,\n",
       "          1.7894e-01, -6.8302e-02,  3.7668e-01, -1.0235e-01,  1.6154e-01,\n",
       "         -1.2001e-01,  6.4618e-02,  3.7400e-01, -2.1962e-02, -2.0386e-01,\n",
       "         -3.3169e-01, -6.6689e-02, -2.2303e-01, -8.9338e-02,  9.3512e-02,\n",
       "         -8.8176e-02, -2.4205e-01, -1.6902e-01, -4.2835e-02,  5.4131e-02,\n",
       "          1.1691e-01,  9.3420e-03, -3.5031e-01, -2.9830e-02, -3.4725e-02,\n",
       "          4.1151e-01,  6.8865e-02, -3.3051e-01, -7.7403e-02,  6.6910e-02,\n",
       "          5.8432e-02, -5.8579e-02,  3.4854e-01, -6.1885e-02, -8.6826e-02,\n",
       "         -4.0627e-02, -1.6122e-01,  1.0210e-01, -1.6870e-01, -5.0255e-02,\n",
       "         -3.6366e-01,  9.2760e-03, -2.6264e-02, -1.7648e-01,  1.2394e-01,\n",
       "          3.5736e-01, -6.1163e-02,  2.3204e-01,  1.1669e-01,  2.5794e-01,\n",
       "          2.7327e-01, -1.9689e-01,  4.5605e-02, -6.1092e-02,  4.5108e-02,\n",
       "          2.8681e-01, -3.5564e-01,  2.6527e-01,  6.4159e-02,  1.6577e-02,\n",
       "          7.1236e-02,  1.0710e-01, -1.5711e-01,  1.6935e-01, -4.8671e-02,\n",
       "          2.7876e-01,  8.3512e-02,  4.6171e-01, -1.2556e-01, -6.0649e-02,\n",
       "         -7.6477e-02,  1.3904e-01, -2.2362e-01, -2.1892e-01,  2.1720e-01,\n",
       "          3.3660e-01, -4.7570e-01, -2.3806e-01, -2.6005e-02,  8.9450e-02,\n",
       "         -1.2443e-02, -2.0270e-01,  2.0246e-01, -1.2755e-01,  1.7464e-01,\n",
       "         -2.9403e-01, -3.8255e-02, -1.0750e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaModel, AutoTokenizer\n",
    "model_checkpoint = 'vinai/phobert-base'\n",
    "# model = QAModule(model_checkpoint=model_checkpoint)\n",
    "model = RobertaModel.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "input = tokenizer(\"DIT CON ME\", return_tensors='pt',\n",
    "        max_length=259,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",)\n",
    "model(input_ids=input.input_ids, attention_mask=input.attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73fdcbcaa6b22d852c0f9bd9783ab6b1b1c25c52a0a8da76beae07513436cb85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
