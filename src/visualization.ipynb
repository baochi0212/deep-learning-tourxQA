{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(32, 8)\n",
    "b = torch.zeros([32, 8], dtype=torch.float)\n",
    "b[0, 0] = 1\n",
    "loss = nn.BCEWithLogitsLoss(reduction='none')(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.where(b != 0, loss, torch.tensor([0.]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.where(b != 0, loss, loss*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 5  # number of tags is 5\n",
    "model = CRF(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "emissions = torch.randn(seq_length, batch_size, num_tags) #n_seq x batch x logits\n",
    "tags = torch.tensor([\n",
    "  [0, 1], [2, 4], [3, 1]\n",
    "], dtype=torch.long)  #nseq x batch\n",
    "loss = model(emissions, tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(30, 32, 142)\n",
    "b = torch.zeros(30, 32, dtype=torch.long)\n",
    "model = CRF(a.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4717.1206, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "name = ['train', 'test', 'dev']\n",
    "for i in name:\n",
    "    for label in open(\"/home/xps/educate/code/hust/XQA/docs/src/JointIDSF/PhoATIS/word-level\" + f'/{i}/label'):\n",
    "        if label.strip() not in list:\n",
    "            list.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_train.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_flight_time', 'atis_airfare', 'atis_aircraft',\n",
       "        'atis_ground_service', 'atis_airline', 'atis_abbreviation',\n",
       "        'atis_quantity'], dtype=object),\n",
       " 4834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'vietanh': 'ngu'}\n",
    "a.update({'vie anh': 'ocho'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_airfare', 'atis_ground_service',\n",
       "        'atis_airline', 'atis_flight_time', 'atis_quantity',\n",
       "        'atis_abbreviation', 'atis_aircraft'], dtype=object),\n",
       " 800)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_test.csv\", header=None)\n",
    "test[0].unique(), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tôi muốn một chuyến bay khứ_hồi từ đồng_hới đến côn_đảo khởi_hành vào thứ tư ngày 16 tháng 6 đến côn_đảo vào buổi tối khoảng 7 giờ và trở về đồng_hới vào buổi tối ngày hôm sau khoảng 7 giờ hãng hàng_không nào có chuyến bay phù_hợp với tuyến đường đó\n"
     ]
    }
   ],
   "source": [
    "lengths = {}\n",
    "with open(\"/home/xps/educate/code/hust/XQA/data/raw/PhoATIS/word-level/train/seq.in\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line.strip().split(' ')) not in lengths.keys():\n",
    "            lengths[len(line.strip().split(' '))] = 1\n",
    "        else:\n",
    "            lengths[len(line.strip().split(' '))] += 1 \n",
    "        if len(line.strip().split(' ')) == 50:\n",
    "            print(line.strip()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lengths, key = lambda d: lengths[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoctro'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename('/hoa/hoctro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "os.environ['dir'] = \"/home/xps/educate/code/hust/XQA/data\"\n",
    "os.environ['src'] = \"/home/xps/educate/code/hust/XQA/src\"\n",
    "import sys\n",
    "\n",
    "from utils.preprocess import string2list, get_label\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from dataset.test_dataset import QADataset\n",
    "\n",
    "data_dir = os.environ['dir']\n",
    "raw_dir = data_dir + '/data/raw/PhoATIS'\n",
    "processed_dir = data_dir + '/data/processed/PhoATIS'\n",
    "qa_processed = data_dir + '/data/processed/QA'\n",
    "tokenizer = AutoTokenizer.from_pretrained('NlpHUST/roberta-base-vn')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('NlpHUST/roberta-base-vn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = QADataset(test_df, tokenizer=tokenizer, MAX_LENGTH=386, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in val_dataset:\n",
    "    if i[2] == 0 and i[3] == 0: \n",
    "        print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[449, 450, 450, 449]\n",
      "[449, 450, 450, 449]\n",
      "[469, 470, 471, 470]\n",
      "[469, 470, 471, 470]\n",
      "[469, 470, 471, 470]\n",
      "[144, 143, 143, 143]\n",
      "[144, 143, 143, 143]\n",
      "[144, 143, 143, 143]\n",
      "[577, 470, 471, 577]\n",
      "[115, 116, 115, 115]\n",
      "[115, 116, 115, 115]\n",
      "[115, 116, 115, 115]\n",
      "[412, 413, 413, 413]\n",
      "[67, 68, 68, 67]\n",
      "[67, 68, 68, 67]\n",
      "[491, 491, 490, 491]\n",
      "[474, 474, 475, 474]\n",
      "[474, 474, 475, 474]\n",
      "[474, 474, 475, 474]\n",
      "[304, 297, 298, 297]\n",
      "[304, 297, 298, 297]\n",
      "[285, 286, 286, 285]\n",
      "[285, 286, 286, 285]\n",
      "[464, 463, 463, 463]\n",
      "[464, 463, 463, 463]\n",
      "[464, 463, 463, 463]\n",
      "[44, 43, 43, 43]\n",
      "[44, 43, 43, 43]\n",
      "[44, 43, 43, 43]\n",
      "[109, 104, 108, 109]\n",
      "[441, 440, 440, 440]\n",
      "[441, 440, 440, 440]\n",
      "[441, 440, 440, 440]\n",
      "[524, 523, 524, 523]\n",
      "[524, 523, 524, 523]\n",
      "[193, 194, 194, 194]\n",
      "[92, 93, 93, 93]\n",
      "[189, 190, 190, 189]\n",
      "[189, 190, 190, 189]\n",
      "[368, 369, 369, 369]\n",
      "[882, 879, 883, 882]\n",
      "[882, 879, 883, 882]\n",
      "[50, 51, 50, 50]\n",
      "[50, 51, 50, 50]\n",
      "[50, 51, 50, 50]\n",
      "[455, 456, 455, 455]\n",
      "[455, 456, 455, 455]\n",
      "[455, 456, 455, 455]\n",
      "[613, 613, 614, 613]\n",
      "[613, 613, 614, 613]\n",
      "[613, 613, 614, 613]\n",
      "[106, 105, 105, 105]\n",
      "[106, 105, 105, 105]\n",
      "[106, 105, 105, 105]\n",
      "[419, 420, 420, 420]\n",
      "[51, 118, 52, 51]\n",
      "[51, 118, 52, 51]\n",
      "[340, 341, 347, 340]\n",
      "[340, 341, 347, 340]\n",
      "[197, 198, 198, 198]\n",
      "[265, 266, 266, 266]\n",
      "[422, 428, 427, 428]\n",
      "[699, 700, 700, 700]\n",
      "[691, 692, 692, 692]\n",
      "[531, 541, 532, 541]\n",
      "[833, 834, 834, 834]\n",
      "[723, 724, 724, 724]\n",
      "[336, 366, 337, 336]\n",
      "[336, 366, 337, 336]\n",
      "[654, 655, 655, 655]\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "for i in range(len(dev_df)):\n",
    "    q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "    a, t = string2list(a, 'int'), string2list(t)\n",
    "    for i in range(len(a)):\n",
    "        if a[i] + 1 in a:\n",
    "            print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Nổi tiếng với tên gọi Kinh đô ánh sáng, Paris là một trung tâm văn hóa lớn của thế giới và cũng là một trong những thành phố du lịch thu hút nhất. Sự nhộn nhịp, các công trình kiến trúc và không khí nghệ sĩ đã giúp Paris mỗi năm có đến 30 triệu khách nước ngoài. Thành phố còn được xem như kinh đô của thời trang cao cấp với nhiều khu phố xa xỉ cùng các trung tâm thương mại lớn. Là nơi đặt trụ sở chính của các tổ chức quốc tế như OECD, UNESCO... cộng với những hoạt động đa dạng về tài chính, kinh doanh, chính trị và du lịch đã khiến Paris trở thành một trong những trung tâm trung chuyển lớn nhất trên thế giới và được coi như một trong bốn \"thành phố toàn cầu\" cùng với New York, Luân Đôn và Tokyo.',\n",
       " '@@@mỗi năm có đến 30 triệu khách nước ngoài@@@Kinh đô ánh sáng@@@Kinh đô ánh sáng@@@Kinh đô ánh sáng',\n",
       " '[221, 22, 22, 22]',\n",
       " 'Điều gì đã nói lên Paris là thành phố lý tưởng để khách du lịch?')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "c, t, a, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label(input, text, start):\n",
    "\n",
    "#     sequence_ids = input.sequence_ids()\n",
    "#     idx = 0\n",
    "#     while sequence_ids[idx] != 1:\n",
    "#             idx += 1\n",
    "#     context_start = idx\n",
    "#     while sequence_ids[idx] == 1:\n",
    "#             idx += 1\n",
    "#     context_end = idx - 1\n",
    "#     offset = input['offset_mapping']\n",
    "    \n",
    "\n",
    "#     start_positions, end_positions = [], []\n",
    "#     start_positions = []\n",
    "#     end_positions = []\n",
    "#     start_char = start\n",
    "#     end_char = start + len(text)\n",
    "#     offset = input['offset_mapping'][0]\n",
    "#     if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "#         start_positions.append(0)\n",
    "#         end_positions.append(0)\n",
    "#     else:\n",
    "#         # Otherwise it's the start and end token positions\n",
    "#         idx = context_start\n",
    "#         while idx <= context_end and offset[idx][0] <= start_char:\n",
    "#             idx += 1\n",
    "#         start_positions.append(idx - 1)\n",
    "#         idx = context_end\n",
    "#         while idx >= context_start and offset[idx][1] >= end_char:\n",
    "#             idx -= 1\n",
    "#         end_positions.append(idx + 1)\n",
    "\n",
    " \n",
    "#     return torch.tensor(start_positions, dtype=torch.long), torch.tensor(end_positions, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Nơi nào nằm trên kênh Saint-Martin nối liền bồn Villette và thượng lưu sông Seine?',\n",
       " 'Dòng sông Seine chảy qua Paris theo hình một cánh cung: vào thành phố từ phía đông nam và ra khỏi thành phố phía tây bắc. Hơn 30 cây cầu của Paris bắc qua dòng sông này. Còn có hai dòng chảy khác qua Paris. Sông Bièvre phía nam, ngày nay ngầm hoàn toàn dưới đất. Kênh Saint-Martin hoàn thành 1825 dài 4,5 km, nối với bồn Villette và vào thành phố ở phía đông bắc. Kênh Saint-Martin chảy ngầm dưới đất cho tới phố Faubourg-du-Temple ở quảng trường Bastille rồi tiếp tục chảy lộ thiên và nối với sông Seine ở phía thượng lưu của đảo Île Saint-Louis. Một con kênh khác là Saint-Denis, cũng nối với bồn Villette theo hướng Saint-Denis, dài 4,5 km và hoàn thành vào năm 1821. Con kênh này gặp sông Seine ở phần hạ lưu và không đi qua Paris. Một dòng sông nữa là Marne, chảy gần Paris qua Seine-Saint-Denis, Val-de-Marne và gặp sông Seine ở phía đông nam thành phố.',\n",
       " 'phố Faubourg-du-Temple ở quảng trường Bastille',\n",
       " 409,\n",
       " 'phố Faubourg-du-Temple ở quảng trường Bastille rồi tiếp tục chảy lộ thiên và nối với sông Seine ở phía thượng lưu của đảo Île Saint-Louis. Một con kênh khác là Saint-Denis, cũng nối với bồn Villette theo hướng Saint-Denis, dài 4,5 km và hoàn thành vào năm 1821. Con kênh này gặp sông Seine ở phần hạ lưu và không đi qua Paris. Một dòng sông nữa là Marne, chảy gần Paris qua Seine-Saint-Denis, Val-de-Marne và gặp sông Seine ở phía đông nam thành phố.')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 32\n",
    "q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "input = tokenizer(q.strip(), c, return_tensors='pt',\n",
    "            max_length=500,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",)\n",
    "a, t = string2list(a, 'int'), string2list(t)\n",
    "q, c, t[0], a[0], c[a[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2idx(start, end, context):\n",
    "    subtext = context[start:end+1]\n",
    "    subtext2 = context[start:end+2]\n",
    "    print(context.split(), subtext.split())\n",
    "    for i in range(len(context.split())):\n",
    "        if context.split()[i] == subtext.split()[0] and context.split()[i+len(subtext.split())-1] == subtext.split()[-1]:\n",
    "            return i, i+len(subtext.split()) - 1\n",
    "        elif context.split()[i] == subtext2.split()[0] and context.split()[i+len(subtext2.split())-1] == subtext2.split()[-1]:\n",
    "            return i, i+len(subtext2.split()) - 1 \n",
    "def offset2length(offset_map):\n",
    "    word_lengths = []\n",
    "    length = 1\n",
    "    offset_map = [offset_map[i] for i in range(len(offset_map)) if offset_map[i].sum() != 0]\n",
    "    for idx in range(1, len(offset_map)):\n",
    "        \n",
    "\n",
    "        if offset_map[idx][0] == offset_map[idx-1][1]:\n",
    "            length += 1\n",
    "        else:\n",
    "            \n",
    "            word_lengths.append(length)\n",
    "            length = 1\n",
    "    word_lengths.append(length)\n",
    "            \n",
    "    return word_lengths\n",
    "def QA_metrics(start, end, start_idx, end_idx, input, tokenizer):\n",
    "    '''\n",
    "    EM and F1 score for text output\n",
    "    start = b x n\n",
    "    '''\n",
    "    EM = 0\n",
    "    F1 = 0\n",
    "    for i in range(start.shape[0]):\n",
    "        pred = tokenizer.decode(input.input_ids[0][start[i]:end[i]+1])\n",
    "        true = tokenizer.decode(input.input_ids[0][start_idx[i]:end_idx[i]+1])\n",
    "        if pred == true:\n",
    "            EM += 1 \n",
    "        sum = 0\n",
    "        text = pred if len(pred.split()) < len(true.split()) else true\n",
    "        for i in range(len(text.split())):\n",
    "            if pred.split()[i] == true.split()[i]:\n",
    "                sum += 1 \n",
    "        precision = sum/len(pred.split())\n",
    "        recall = sum/len(true.split())\n",
    "        F1 += 2/(1/precision + 1/recall)\n",
    "    return EM/start.shape[0], F1/start.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "def get_label(input, text, start, reverse=False, max_length=300, context=None, question=None):\n",
    "    '''\n",
    "    we can make use of original sequence or sub_word sequence (tokenized for labelling)\n",
    "    - non-reverse: use the mapping to map start and end idx\n",
    "    - reverse: og start and end idx, and word lengths for mapping, use context for get start and end\n",
    "    '''\n",
    "\n",
    "    if reverse:\n",
    "        start_positions, end_positions = start, start + len(text) - 1\n",
    "        start_positions, end_positions = char2idx(start_positions, end_positions, context)\n",
    "        start_positions += len(question.split())\n",
    "        end_positions += len(question.split())\n",
    "        offset_mapping = input.offset_mapping[0]\n",
    "        word_lengths = offset2length(offset_mapping)\n",
    "        # while len(word_lengths) < max_length:\n",
    "        #     word_lengths.append(1)\n",
    "        return torch.tensor(start_positions, dtype=torch.long), torch.tensor(end_positions, dtype=torch.long), torch.tensor(word_lengths, dtype=torch.long)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    sequence_ids = input.sequence_ids()\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "    context_end = idx - 1\n",
    "    offset = input['offset_mapping']\n",
    "    \n",
    "\n",
    "    start_positions, end_positions = 0, 0\n",
    "    start_char = start\n",
    "    end_char = start + len(text)\n",
    "    offset = input['offset_mapping'][0]\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions = 0\n",
    "        end_positions = 0\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions = idx - 1\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions = idx + 1\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "    return torch.tensor(start_positions, dtype=torch.long), torch.tensor(end_positions, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dev_df)):\n",
    "#     q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "#     input = tokenizer(q.strip(), c, return_tensors='pt',\n",
    "#                 max_length=500,\n",
    "#                 truncation=\"only_second\",\n",
    "#                 return_offsets_mapping=True,\n",
    "#                 padding=\"max_length\",)\n",
    "#     a, t = string2list(a, 'int'), string2list(t)\n",
    "#     w = offset2length(input.offset_mapping[0])\n",
    "#     if len(w) != len((q + \" \" + c).split()):\n",
    "#         print(i, len(w), len((q + \" \" + c).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens(word_lengths, input):\n",
    "    # b x w x n\n",
    "    batch_size = word_lengths.shape[0]\n",
    "    word_size = word_lengths.shape[1]\n",
    "    token_size = input.input_ids.shape[1]\n",
    "    align_matrix = torch.zeros(batch_size, word_size, token_size)\n",
    "    # align\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(word_size):\n",
    "            # fill till finishing the word\n",
    "            temp, idx  = word_lengths[i, j], 0\n",
    "            while temp > 0:\n",
    "                if input.offset_mapping[i, idx].sum() != 0:\n",
    "                    align_matrix[i, j, idx] = 1\n",
    "                    temp -= 1 \n",
    "                idx += 1 \n",
    "    return align_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['Paris', 'nằm', 'ở', 'điểm', 'gặp', 'nhau', 'của', 'các', 'hành', 'trình', 'thương', 'mại', 'đường', 'bộ', 'và', 'đường', 'sông,', 'và', 'là', 'trung', 'tâm', 'của', 'một', 'vùng', 'nông', 'nghiệp', 'giàu', 'có.', 'Vào', 'thế', 'kỷ', '10,', 'Paris', 'đã', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'chính', 'của', 'Pháp', 'cùng', 'các', 'cung', 'điện', 'hoàng', 'gia,', 'các', 'tu', 'viện', 'và', 'nhà', 'thờ.', 'Từ', 'thế', 'kỷ', '12,', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'của', 'châu', 'Âu', 'về', 'giáo', 'dục', 'và', 'nghệ', 'thuật.', 'Thế', 'kỷ', '14,', 'Paris', 'là', 'thành', 'phố', 'quan', 'trọng', 'bậc', 'nhất', 'của', 'Cơ', 'Đốc', 'giáo', 'và', 'trong', 'các', 'thế', 'kỷ', '16,', '17,', 'đây', 'là', 'nơi', 'diễn', 'ra', 'Cách', 'mạng', 'Pháp', 'cùng', 'nhiều', 'sự', 'kiện', 'lịch', 'sử', 'quan', 'trọng', 'của', 'Pháp', 'và', 'châu', 'Âu.', 'Đến', 'thế', 'kỷ', '19', 'và', '20,', 'thành', 'phố', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'văn', 'hóa', 'của', 'thế', 'giới,', 'thủ', 'đô', 'của', 'nghệ', 'thuật', 'và', 'giải', 'trí.'] ['nằm', 'ở', 'điểm', 'gặp', 'nhau', 'của', 'các', 'hành', 'trình', 'thương', 'mại', 'đường', 'bộ', 'và', 'đường', 'sông']\n",
      "1\n",
      "['Paris', 'nằm', 'ở', 'điểm', 'gặp', 'nhau', 'của', 'các', 'hành', 'trình', 'thương', 'mại', 'đường', 'bộ', 'và', 'đường', 'sông,', 'và', 'là', 'trung', 'tâm', 'của', 'một', 'vùng', 'nông', 'nghiệp', 'giàu', 'có.', 'Vào', 'thế', 'kỷ', '10,', 'Paris', 'đã', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'chính', 'của', 'Pháp', 'cùng', 'các', 'cung', 'điện', 'hoàng', 'gia,', 'các', 'tu', 'viện', 'và', 'nhà', 'thờ.', 'Từ', 'thế', 'kỷ', '12,', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'của', 'châu', 'Âu', 'về', 'giáo', 'dục', 'và', 'nghệ', 'thuật.', 'Thế', 'kỷ', '14,', 'Paris', 'là', 'thành', 'phố', 'quan', 'trọng', 'bậc', 'nhất', 'của', 'Cơ', 'Đốc', 'giáo', 'và', 'trong', 'các', 'thế', 'kỷ', '16,', '17,', 'đây', 'là', 'nơi', 'diễn', 'ra', 'Cách', 'mạng', 'Pháp', 'cùng', 'nhiều', 'sự', 'kiện', 'lịch', 'sử', 'quan', 'trọng', 'của', 'Pháp', 'và', 'châu', 'Âu.', 'Đến', 'thế', 'kỷ', '19', 'và', '20,', 'thành', 'phố', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'văn', 'hóa', 'của', 'thế', 'giới,', 'thủ', 'đô', 'của', 'nghệ', 'thuật', 'và', 'giải', 'trí.'] ['nông', 'nghiệp']\n",
      "2\n",
      "['Paris', 'nằm', 'ở', 'điểm', 'gặp', 'nhau', 'của', 'các', 'hành', 'trình', 'thương', 'mại', 'đường', 'bộ', 'và', 'đường', 'sông,', 'và', 'là', 'trung', 'tâm', 'của', 'một', 'vùng', 'nông', 'nghiệp', 'giàu', 'có.', 'Vào', 'thế', 'kỷ', '10,', 'Paris', 'đã', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'chính', 'của', 'Pháp', 'cùng', 'các', 'cung', 'điện', 'hoàng', 'gia,', 'các', 'tu', 'viện', 'và', 'nhà', 'thờ.', 'Từ', 'thế', 'kỷ', '12,', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'của', 'châu', 'Âu', 'về', 'giáo', 'dục', 'và', 'nghệ', 'thuật.', 'Thế', 'kỷ', '14,', 'Paris', 'là', 'thành', 'phố', 'quan', 'trọng', 'bậc', 'nhất', 'của', 'Cơ', 'Đốc', 'giáo', 'và', 'trong', 'các', 'thế', 'kỷ', '16,', '17,', 'đây', 'là', 'nơi', 'diễn', 'ra', 'Cách', 'mạng', 'Pháp', 'cùng', 'nhiều', 'sự', 'kiện', 'lịch', 'sử', 'quan', 'trọng', 'của', 'Pháp', 'và', 'châu', 'Âu.', 'Đến', 'thế', 'kỷ', '19', 'và', '20,', 'thành', 'phố', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'văn', 'hóa', 'của', 'thế', 'giới,', 'thủ', 'đô', 'của', 'nghệ', 'thuật', 'và', 'giải', 'trí.'] ['Paris']\n",
      "3\n",
      "['Paris', 'nằm', 'ở', 'điểm', 'gặp', 'nhau', 'của', 'các', 'hành', 'trình', 'thương', 'mại', 'đường', 'bộ', 'và', 'đường', 'sông,', 'và', 'là', 'trung', 'tâm', 'của', 'một', 'vùng', 'nông', 'nghiệp', 'giàu', 'có.', 'Vào', 'thế', 'kỷ', '10,', 'Paris', 'đã', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'chính', 'của', 'Pháp', 'cùng', 'các', 'cung', 'điện', 'hoàng', 'gia,', 'các', 'tu', 'viện', 'và', 'nhà', 'thờ.', 'Từ', 'thế', 'kỷ', '12,', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'của', 'châu', 'Âu', 'về', 'giáo', 'dục', 'và', 'nghệ', 'thuật.', 'Thế', 'kỷ', '14,', 'Paris', 'là', 'thành', 'phố', 'quan', 'trọng', 'bậc', 'nhất', 'của', 'Cơ', 'Đốc', 'giáo', 'và', 'trong', 'các', 'thế', 'kỷ', '16,', '17,', 'đây', 'là', 'nơi', 'diễn', 'ra', 'Cách', 'mạng', 'Pháp', 'cùng', 'nhiều', 'sự', 'kiện', 'lịch', 'sử', 'quan', 'trọng', 'của', 'Pháp', 'và', 'châu', 'Âu.', 'Đến', 'thế', 'kỷ', '19', 'và', '20,', 'thành', 'phố', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'văn', 'hóa', 'của', 'thế', 'giới,', 'thủ', 'đô', 'của', 'nghệ', 'thuật', 'và', 'giải', 'trí.'] ['nghệ', 'thuật', 'và', 'giải', 'trí']\n",
      "4\n",
      "['Paris', 'nằm', 'ở', 'điểm', 'gặp', 'nhau', 'của', 'các', 'hành', 'trình', 'thương', 'mại', 'đường', 'bộ', 'và', 'đường', 'sông,', 'và', 'là', 'trung', 'tâm', 'của', 'một', 'vùng', 'nông', 'nghiệp', 'giàu', 'có.', 'Vào', 'thế', 'kỷ', '10,', 'Paris', 'đã', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'chính', 'của', 'Pháp', 'cùng', 'các', 'cung', 'điện', 'hoàng', 'gia,', 'các', 'tu', 'viện', 'và', 'nhà', 'thờ.', 'Từ', 'thế', 'kỷ', '12,', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'của', 'châu', 'Âu', 'về', 'giáo', 'dục', 'và', 'nghệ', 'thuật.', 'Thế', 'kỷ', '14,', 'Paris', 'là', 'thành', 'phố', 'quan', 'trọng', 'bậc', 'nhất', 'của', 'Cơ', 'Đốc', 'giáo', 'và', 'trong', 'các', 'thế', 'kỷ', '16,', '17,', 'đây', 'là', 'nơi', 'diễn', 'ra', 'Cách', 'mạng', 'Pháp', 'cùng', 'nhiều', 'sự', 'kiện', 'lịch', 'sử', 'quan', 'trọng', 'của', 'Pháp', 'và', 'châu', 'Âu.', 'Đến', 'thế', 'kỷ', '19', 'và', '20,', 'thành', 'phố', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'văn', 'hóa', 'của', 'thế', 'giới,', 'thủ', 'đô', 'của', 'nghệ', 'thuật', 'và', 'giải', 'trí.'] ['thế', 'kỷ', '10']\n",
      "5\n",
      "['Nổi', 'tiếng', 'với', 'tên', 'gọi', 'Kinh', 'đô', 'ánh', 'sáng,', 'Paris', 'là', 'một', 'trung', 'tâm', 'văn', 'hóa', 'lớn', 'của', 'thế', 'giới', 'và', 'cũng', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'du', 'lịch', 'thu', 'hút', 'nhất.', 'Sự', 'nhộn', 'nhịp,', 'các', 'công', 'trình', 'kiến', 'trúc', 'và', 'không', 'khí', 'nghệ', 'sĩ', 'đã', 'giúp', 'Paris', 'mỗi', 'năm', 'có', 'đến', '30', 'triệu', 'khách', 'nước', 'ngoài.', 'Thành', 'phố', 'còn', 'được', 'xem', 'như', 'kinh', 'đô', 'của', 'thời', 'trang', 'cao', 'cấp', 'với', 'nhiều', 'khu', 'phố', 'xa', 'xỉ', 'cùng', 'các', 'trung', 'tâm', 'thương', 'mại', 'lớn.', 'Là', 'nơi', 'đặt', 'trụ', 'sở', 'chính', 'của', 'các', 'tổ', 'chức', 'quốc', 'tế', 'như', 'OECD,', 'UNESCO...', 'cộng', 'với', 'những', 'hoạt', 'động', 'đa', 'dạng', 'về', 'tài', 'chính,', 'kinh', 'doanh,', 'chính', 'trị', 'và', 'du', 'lịch', 'đã', 'khiến', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'trung', 'chuyển', 'lớn', 'nhất', 'trên', 'thế', 'giới', 'và', 'được', 'coi', 'như', 'một', 'trong', 'bốn', '\"thành', 'phố', 'toàn', 'cầu\"', 'cùng', 'với', 'New', 'York,', 'Luân', 'Đôn', 'và', 'Tokyo.'] ['mỗi', 'năm', 'có', 'đến', '30', 'triệu', 'khách', 'nước', 'ngoài']\n",
      "6\n",
      "['Nổi', 'tiếng', 'với', 'tên', 'gọi', 'Kinh', 'đô', 'ánh', 'sáng,', 'Paris', 'là', 'một', 'trung', 'tâm', 'văn', 'hóa', 'lớn', 'của', 'thế', 'giới', 'và', 'cũng', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'du', 'lịch', 'thu', 'hút', 'nhất.', 'Sự', 'nhộn', 'nhịp,', 'các', 'công', 'trình', 'kiến', 'trúc', 'và', 'không', 'khí', 'nghệ', 'sĩ', 'đã', 'giúp', 'Paris', 'mỗi', 'năm', 'có', 'đến', '30', 'triệu', 'khách', 'nước', 'ngoài.', 'Thành', 'phố', 'còn', 'được', 'xem', 'như', 'kinh', 'đô', 'của', 'thời', 'trang', 'cao', 'cấp', 'với', 'nhiều', 'khu', 'phố', 'xa', 'xỉ', 'cùng', 'các', 'trung', 'tâm', 'thương', 'mại', 'lớn.', 'Là', 'nơi', 'đặt', 'trụ', 'sở', 'chính', 'của', 'các', 'tổ', 'chức', 'quốc', 'tế', 'như', 'OECD,', 'UNESCO...', 'cộng', 'với', 'những', 'hoạt', 'động', 'đa', 'dạng', 'về', 'tài', 'chính,', 'kinh', 'doanh,', 'chính', 'trị', 'và', 'du', 'lịch', 'đã', 'khiến', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'trung', 'chuyển', 'lớn', 'nhất', 'trên', 'thế', 'giới', 'và', 'được', 'coi', 'như', 'một', 'trong', 'bốn', '\"thành', 'phố', 'toàn', 'cầu\"', 'cùng', 'với', 'New', 'York,', 'Luân', 'Đôn', 'và', 'Tokyo.'] ['Sự', 'nhộn', 'nhịp,', 'các', 'công', 'trình', 'kiến', 'trúc', 'và', 'không', 'khí', 'nghệ', 'sĩ']\n",
      "7\n",
      "['Nổi', 'tiếng', 'với', 'tên', 'gọi', 'Kinh', 'đô', 'ánh', 'sáng,', 'Paris', 'là', 'một', 'trung', 'tâm', 'văn', 'hóa', 'lớn', 'của', 'thế', 'giới', 'và', 'cũng', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'du', 'lịch', 'thu', 'hút', 'nhất.', 'Sự', 'nhộn', 'nhịp,', 'các', 'công', 'trình', 'kiến', 'trúc', 'và', 'không', 'khí', 'nghệ', 'sĩ', 'đã', 'giúp', 'Paris', 'mỗi', 'năm', 'có', 'đến', '30', 'triệu', 'khách', 'nước', 'ngoài.', 'Thành', 'phố', 'còn', 'được', 'xem', 'như', 'kinh', 'đô', 'của', 'thời', 'trang', 'cao', 'cấp', 'với', 'nhiều', 'khu', 'phố', 'xa', 'xỉ', 'cùng', 'các', 'trung', 'tâm', 'thương', 'mại', 'lớn.', 'Là', 'nơi', 'đặt', 'trụ', 'sở', 'chính', 'của', 'các', 'tổ', 'chức', 'quốc', 'tế', 'như', 'OECD,', 'UNESCO...', 'cộng', 'với', 'những', 'hoạt', 'động', 'đa', 'dạng', 'về', 'tài', 'chính,', 'kinh', 'doanh,', 'chính', 'trị', 'và', 'du', 'lịch', 'đã', 'khiến', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'trung', 'chuyển', 'lớn', 'nhất', 'trên', 'thế', 'giới', 'và', 'được', 'coi', 'như', 'một', 'trong', 'bốn', '\"thành', 'phố', 'toàn', 'cầu\"', 'cùng', 'với', 'New', 'York,', 'Luân', 'Đôn', 'và', 'Tokyo.'] ['Thành', 'phố', 'còn', 'được', 'xem', 'như', 'kinh', 'đô', 'của', 'thời', 'trang', 'cao', 'cấp', 'với', 'nhiều', 'khu', 'phố', 'xa', 'xỉ', 'cùng', 'các', 'trung', 'tâm', 'thương', 'mại', 'lớn']\n",
      "8\n",
      "['Nổi', 'tiếng', 'với', 'tên', 'gọi', 'Kinh', 'đô', 'ánh', 'sáng,', 'Paris', 'là', 'một', 'trung', 'tâm', 'văn', 'hóa', 'lớn', 'của', 'thế', 'giới', 'và', 'cũng', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'du', 'lịch', 'thu', 'hút', 'nhất.', 'Sự', 'nhộn', 'nhịp,', 'các', 'công', 'trình', 'kiến', 'trúc', 'và', 'không', 'khí', 'nghệ', 'sĩ', 'đã', 'giúp', 'Paris', 'mỗi', 'năm', 'có', 'đến', '30', 'triệu', 'khách', 'nước', 'ngoài.', 'Thành', 'phố', 'còn', 'được', 'xem', 'như', 'kinh', 'đô', 'của', 'thời', 'trang', 'cao', 'cấp', 'với', 'nhiều', 'khu', 'phố', 'xa', 'xỉ', 'cùng', 'các', 'trung', 'tâm', 'thương', 'mại', 'lớn.', 'Là', 'nơi', 'đặt', 'trụ', 'sở', 'chính', 'của', 'các', 'tổ', 'chức', 'quốc', 'tế', 'như', 'OECD,', 'UNESCO...', 'cộng', 'với', 'những', 'hoạt', 'động', 'đa', 'dạng', 'về', 'tài', 'chính,', 'kinh', 'doanh,', 'chính', 'trị', 'và', 'du', 'lịch', 'đã', 'khiến', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'trung', 'chuyển', 'lớn', 'nhất', 'trên', 'thế', 'giới', 'và', 'được', 'coi', 'như', 'một', 'trong', 'bốn', '\"thành', 'phố', 'toàn', 'cầu\"', 'cùng', 'với', 'New', 'York,', 'Luân', 'Đôn', 'và', 'Tokyo.'] ['New', 'York,', 'Luân', 'Đôn', 'và', 'Tokyo']\n",
      "9\n",
      "['Nổi', 'tiếng', 'với', 'tên', 'gọi', 'Kinh', 'đô', 'ánh', 'sáng,', 'Paris', 'là', 'một', 'trung', 'tâm', 'văn', 'hóa', 'lớn', 'của', 'thế', 'giới', 'và', 'cũng', 'là', 'một', 'trong', 'những', 'thành', 'phố', 'du', 'lịch', 'thu', 'hút', 'nhất.', 'Sự', 'nhộn', 'nhịp,', 'các', 'công', 'trình', 'kiến', 'trúc', 'và', 'không', 'khí', 'nghệ', 'sĩ', 'đã', 'giúp', 'Paris', 'mỗi', 'năm', 'có', 'đến', '30', 'triệu', 'khách', 'nước', 'ngoài.', 'Thành', 'phố', 'còn', 'được', 'xem', 'như', 'kinh', 'đô', 'của', 'thời', 'trang', 'cao', 'cấp', 'với', 'nhiều', 'khu', 'phố', 'xa', 'xỉ', 'cùng', 'các', 'trung', 'tâm', 'thương', 'mại', 'lớn.', 'Là', 'nơi', 'đặt', 'trụ', 'sở', 'chính', 'của', 'các', 'tổ', 'chức', 'quốc', 'tế', 'như', 'OECD,', 'UNESCO...', 'cộng', 'với', 'những', 'hoạt', 'động', 'đa', 'dạng', 'về', 'tài', 'chính,', 'kinh', 'doanh,', 'chính', 'trị', 'và', 'du', 'lịch', 'đã', 'khiến', 'Paris', 'trở', 'thành', 'một', 'trong', 'những', 'trung', 'tâm', 'trung', 'chuyển', 'lớn', 'nhất', 'trên', 'thế', 'giới', 'và', 'được', 'coi', 'như', 'một', 'trong', 'bốn', '\"thành', 'phố', 'toàn', 'cầu\"', 'cùng', 'với', 'New', 'York,', 'Luân', 'Đôn', 'và', 'Tokyo.'] ['Là', 'nơi', 'đặt', 'trụ', 'sở', 'chính', 'của', 'các', 'tổ', 'chức', 'quốc', 'tế', 'như', 'OECD,', 'UNESCO...', 'cộng', 'với', 'những', 'hoạt', 'động', 'đa', 'dạng', 'về', 'tài', 'chính,', 'kinh', 'doanh,', 'chính', 'trị', 'và', 'du', 'lịch']\n",
      "10\n",
      "['Paris', 'nổi', 'tiếng', 'với', 'tên', 'gọi', '\"Kinh', 'đô', 'ánh', 'sáng\",', 'vốn', 'từ', 'trong', 'tiếng', 'Pháp', 'là', '\"Ville', 'lumière\",', 'dịch', 'chính', 'xác', 'là', 'Thành', 'phố', 'ánh', 'sáng,', 'cũng', 'giống', 'trong', 'tiếng', 'Anh:', 'The', 'City', 'of', 'Lights.', 'Tên', 'gọi', 'này', 'được', 'bắt', 'đầu', 'từ', 'nghĩa', 'đen', 'của', 'nó:', 'cuối', 'thế', 'kỷ', '17,', 'trung', 'tướng', 'cảnh', 'sát', 'đầu', 'tiên', 'của', 'Paris', 'Gabriel', 'Nicolas', 'de', 'La', 'Reynie', 'ra', 'lệnh', 'thắp', 'sáng', 'những', 'khu', 'vực', 'công', 'cộng', 'nhiều', 'tệ', 'nạn', 'của', 'thành', 'phố.', 'Nhưng', 'bởi', 'Paris', 'nổi', 'tiếng', 'với', 'vị', 'trí', 'trung', 'tâm', 'văn', 'hóa,', 'tri', 'thức', 'của', 'cả', 'thế', 'giới,', 'nên', 'tên', 'gọi', 'này', 'thường', 'được', 'hiểu', 'theo', 'nghĩa', 'bóng.'] ['Kinh', 'đô', 'ánh', 'sáng']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xps/educate/code/hust/XQA/src/visualization.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m a, t \u001b[39m=\u001b[39m string2list(a, \u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m), string2list(t)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m _, _, w \u001b[39m=\u001b[39m get_label(\u001b[39minput\u001b[39;49m, t[\u001b[39m0\u001b[39;49m], a[\u001b[39m0\u001b[39;49m], reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, context\u001b[39m=\u001b[39;49mc, question\u001b[39m=\u001b[39;49mq)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m((q \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m c)\u001b[39m.\u001b[39msplit()):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "\u001b[1;32m/home/xps/educate/code/hust/XQA/src/visualization.ipynb Cell 30\u001b[0m in \u001b[0;36mget_label\u001b[0;34m(input, text, start, reverse, max_length, context, question)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mif\u001b[39;00m reverse:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     start_positions, end_positions \u001b[39m=\u001b[39m start, start \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(text) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     start_positions, end_positions \u001b[39m=\u001b[39m char2idx(start_positions, end_positions, context)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     start_positions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(question\u001b[39m.\u001b[39msplit())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     end_positions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(question\u001b[39m.\u001b[39msplit())\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for i in range(len(dev_df)):\n",
    "    q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "    input = tokenizer(q.strip(), c, return_tensors='pt',\n",
    "                    max_length=512,\n",
    "                    truncation=\"only_second\",\n",
    "                    return_offsets_mapping=True,\n",
    "                    padding=\"max_length\",)\n",
    "    a, t = string2list(a, 'int'), string2list(t)\n",
    "    print(i)\n",
    "    _, _, w = get_label(input, t[0], a[0], reverse=True, context=c, question=q)\n",
    "    if len(w) != len((q + \" \"+ c).split()):\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'nổi', 'tiếng', 'với', 'tên', 'gọi', '\"Kinh', 'đô', 'ánh', 'sáng\",', 'vốn', 'từ', 'trong', 'tiếng', 'Pháp', 'là', '\"Ville', 'lumière\",', 'dịch', 'chính', 'xác', 'là', 'Thành', 'phố', 'ánh', 'sáng,', 'cũng', 'giống', 'trong', 'tiếng', 'Anh:', 'The', 'City', 'of', 'Lights.', 'Tên', 'gọi', 'này', 'được', 'bắt', 'đầu', 'từ', 'nghĩa', 'đen', 'của', 'nó:', 'cuối', 'thế', 'kỷ', '17,', 'trung', 'tướng', 'cảnh', 'sát', 'đầu', 'tiên', 'của', 'Paris', 'Gabriel', 'Nicolas', 'de', 'La', 'Reynie', 'ra', 'lệnh', 'thắp', 'sáng', 'những', 'khu', 'vực', 'công', 'cộng', 'nhiều', 'tệ', 'nạn', 'của', 'thành', 'phố.', 'Nhưng', 'bởi', 'Paris', 'nổi', 'tiếng', 'với', 'vị', 'trí', 'trung', 'tâm', 'văn', 'hóa,', 'tri', 'thức', 'của', 'cả', 'thế', 'giới,', 'nên', 'tên', 'gọi', 'này', 'thường', 'được', 'hiểu', 'theo', 'nghĩa', 'bóng.'] ['Kinh', 'đô', 'ánh', 'sáng\"']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Người dân trên thế giới thường gọi Paris với tên khác là gì? Paris nổi tiếng với tên gọi \"Kinh đô ánh sáng\", vốn từ trong tiếng Pháp là \"Ville lumière\", dịch chính xác là Thành phố ánh sáng, cũng giống trong tiếng Anh: The City of Lights. Tên gọi này được bắt đầu từ nghĩa đen của nó: cuối thế kỷ 17, trung tướng cảnh sát đầu tiên của Paris Gabriel Nicolas de La Reynie ra lệnh thắp sáng những khu vực công cộng nhiều tệ nạn của thành phố. Nhưng bởi Paris nổi tiếng với vị trí trung tâm văn hóa, tri thức của cả thế giới, nên tên gọi này thường được hiểu theo nghĩa bóng.',\n",
       " 'Kinh đô ánh sáng\", vốn từ trong tiếng Pháp là \"Ville lumière\", dịch chính xác là Thành phố ánh sáng, cũng giống trong tiếng Anh: The City of Lights. Tên gọi này được bắt đầu từ nghĩa đen của nó: cuối thế kỷ 17, trung tướng cảnh sát đầu tiên của Paris Gabriel Nicolas de La Reynie ra lệnh thắp sáng những khu vực công cộng nhiều tệ nạn của thành phố. Nhưng bởi Paris nổi tiếng với vị trí trung tâm văn hóa, tri thức của cả thế giới, nên tên gọi này thường được hiểu theo nghĩa bóng.')"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx(a[0], a[0]+len(t[0]), c)\n",
    "(q + ' ' + c), c[a[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = align_tokens(w.unsqueeze(0), input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, torch.Size([1, 184, 512]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((q.strip() + \" \" + c.strip()).split()), matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizerFast' object has no attribute 'special_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/xps/educate/code/hust/XQA/src/visualization.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mspecial_tokens\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'special_tokens'"
     ]
    }
   ],
   "source": [
    "tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "for i in range(len(w)):\n",
    "    word = \"\"\n",
    "    count = 0\n",
    "    length = w[i]\n",
    "    while count < length:\n",
    "        if input.offset_mapping[0][idx] != (0, 0):\n",
    "            s, e = input.offset_mapping[0][idx]\n",
    "            word += text[s:e]\n",
    "            count += 1 \n",
    "        idx += 1\n",
    "    words.append(word)\n",
    "len(words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Seoul': 10853,\n",
       " 'nasty': 13392,\n",
       " 'presented': 2756,\n",
       " 'Lakshmi': 23691,\n",
       " 'manual': 9506,\n",
       " 'Mixing': 21796,\n",
       " 'Maud': 24351,\n",
       " '##gni': 22152,\n",
       " 'textbook': 18977,\n",
       " 'platforms': 6833,\n",
       " '##hiro': 17427,\n",
       " 'kidnapped': 11665,\n",
       " 'auction': 11046,\n",
       " 'conjunction': 9342,\n",
       " 'transporting': 19920,\n",
       " 'Lucifer': 27554,\n",
       " '##%': 28129,\n",
       " '##.': 28138,\n",
       " 'movements': 5172,\n",
       " 'continuity': 15126,\n",
       " 'applicant': 27414,\n",
       " 'accomplishments': 19156,\n",
       " 'Tam': 22876,\n",
       " 'Associate': 9666,\n",
       " 'Riley': 7024,\n",
       " 'paperwork': 18989,\n",
       " '##rera': 25508,\n",
       " 'Ш': 471,\n",
       " '⁰': 800,\n",
       " 'Wang': 7014,\n",
       " '##iculate': 26748,\n",
       " 'pope': 17460,\n",
       " 'Waterford': 19081,\n",
       " 'comprising': 9472,\n",
       " '##rsing': 25351,\n",
       " '##ay': 4164,\n",
       " 'seconds': 3071,\n",
       " '1846': 8911,\n",
       " 'プ': 962,\n",
       " '##egan': 18886,\n",
       " 'compatible': 12173,\n",
       " 'backwards': 11316,\n",
       " '1661': 26981,\n",
       " '##elf': 22775,\n",
       " 'Serbia': 6689,\n",
       " 'Edwin': 9854,\n",
       " 'reservoirs': 26536,\n",
       " 'Move': 15729,\n",
       " 'Dragons': 12230,\n",
       " 'দ': 650,\n",
       " 'symptoms': 8006,\n",
       " 'Shake': 25775,\n",
       " 'investigate': 8242,\n",
       " 'miners': 13418,\n",
       " '##gade': 24003,\n",
       " 'Chinese': 1922,\n",
       " 'Hayden': 14344,\n",
       " 'welfare': 9319,\n",
       " 'IDF': 27140,\n",
       " 'freaking': 14599,\n",
       " 'Based': 7457,\n",
       " 'pulling': 4239,\n",
       " 'armoured': 16267,\n",
       " '##bria': 24911,\n",
       " 'reproduced': 25017,\n",
       " 'Barrett': 12908,\n",
       " '##rrick': 27511,\n",
       " 'Count': 5704,\n",
       " '##let': 5765,\n",
       " '##͡': 28314,\n",
       " 'Plymouth': 10033,\n",
       " 'lenses': 16938,\n",
       " 'movie': 2523,\n",
       " 'patrols': 15803,\n",
       " 'matters': 5218,\n",
       " 'Wyoming': 10513,\n",
       " 'smirk': 15861,\n",
       " 'trillion': 27005,\n",
       " 'aviation': 8747,\n",
       " '##rts': 13245,\n",
       " 'Diana': 8506,\n",
       " 'Somali': 17950,\n",
       " '##ই': 28541,\n",
       " 'Zeus': 16111,\n",
       " 'prominent': 3289,\n",
       " 'identified': 3626,\n",
       " 'Laurel': 13616,\n",
       " '##ell': 3991,\n",
       " 'unified': 13943,\n",
       " '##cale': 20532,\n",
       " '##ney': 4695,\n",
       " 'fund': 5841,\n",
       " 'reform': 5851,\n",
       " '[unused33]': 33,\n",
       " 'Saint': 2216,\n",
       " 'tragedy': 12343,\n",
       " 'garde': 19484,\n",
       " 'conjecture': 26031,\n",
       " 'https': 18630,\n",
       " 'pubs': 27392,\n",
       " 'posthumously': 13165,\n",
       " '##urn': 17384,\n",
       " 'Montgomery': 8079,\n",
       " 'grinned': 6375,\n",
       " '##lean': 21649,\n",
       " 'Vicar': 22269,\n",
       " '##wash': 24745,\n",
       " 'adapted': 5546,\n",
       " 'Stage': 6160,\n",
       " 'graph': 10873,\n",
       " 'spaced': 22445,\n",
       " 'center': 2057,\n",
       " '##med': 4611,\n",
       " 'Gods': 11795,\n",
       " 'icon': 15802,\n",
       " 'vault': 13454,\n",
       " 'Netherlands': 3706,\n",
       " 'submerged': 14203,\n",
       " '##att': 19934,\n",
       " 'systematically': 25923,\n",
       " 'Wade': 11052,\n",
       " 'millennium': 17928,\n",
       " '##nt': 2227,\n",
       " '##à': 9183,\n",
       " 'Re': 11336,\n",
       " 'vessel': 5832,\n",
       " 'version': 1683,\n",
       " 'BT': 27378,\n",
       " 'thinks': 6191,\n",
       " '、': 885,\n",
       " 'intention': 6247,\n",
       " 'preserved': 6018,\n",
       " 'diplomacy': 24500,\n",
       " '##ibes': 20195,\n",
       " 'Botswana': 21139,\n",
       " 'siding': 20181,\n",
       " 'storey': 12758,\n",
       " '##cite': 14375,\n",
       " '##dra': 7412,\n",
       " 'analyzed': 17689,\n",
       " 'norms': 19600,\n",
       " '15': 1405,\n",
       " 'theme': 3815,\n",
       " 'ceramic': 17060,\n",
       " 'move': 1815,\n",
       " 'Being': 6819,\n",
       " 'Sector': 17063,\n",
       " 'betrayed': 12546,\n",
       " 'Antwerp': 14492,\n",
       " 'clarinet': 13866,\n",
       " '##nning': 15918,\n",
       " 'upbringing': 27981,\n",
       " 'pull': 3373,\n",
       " 'general': 1704,\n",
       " 'tearing': 14156,\n",
       " 'enterprise': 10614,\n",
       " 'Nelson': 5232,\n",
       " 'messenger': 17957,\n",
       " 'foothills': 21864,\n",
       " 'calculating': 23172,\n",
       " 'Milan': 6390,\n",
       " 'drains': 20681,\n",
       " 'Exhibition': 9336,\n",
       " 'Blaine': 21849,\n",
       " 'aims': 8469,\n",
       " 'Philipp': 22765,\n",
       " '♭': 880,\n",
       " '##S': 1708,\n",
       " 'subdued': 22566,\n",
       " 'upheld': 17923,\n",
       " '##Ο': 28327,\n",
       " '##ნ': 28605,\n",
       " '##hi': 3031,\n",
       " '##Т': 28383,\n",
       " 'arts': 3959,\n",
       " 'Falls': 6230,\n",
       " 'anguish': 25096,\n",
       " '162': 19163,\n",
       " 'Robert': 1823,\n",
       " 'manipulate': 19109,\n",
       " 'و': 592,\n",
       " 'Navy': 2506,\n",
       " 'pre': 3073,\n",
       " 'Vic': 11600,\n",
       " 'Greenberg': 27174,\n",
       " 'floors': 7849,\n",
       " '##bbled': 14437,\n",
       " 'Network': 3998,\n",
       " 'consulted': 18881,\n",
       " 'hide': 4750,\n",
       " 'Kaufman': 26517,\n",
       " 'enters': 7603,\n",
       " 'Odd': 22161,\n",
       " '[unused40]': 40,\n",
       " '##llow': 24834,\n",
       " 'shaped': 4283,\n",
       " 'Lin': 12221,\n",
       " 'Miriam': 18131,\n",
       " '##cker': 8638,\n",
       " 'compose': 18742,\n",
       " 'Barbados': 18010,\n",
       " 'boom': 11147,\n",
       " 'educators': 24937,\n",
       " 'Montenegrin': 27064,\n",
       " 'Romanized': 7278,\n",
       " 'alcoholic': 16149,\n",
       " 'ghosts': 14586,\n",
       " 'synagogue': 17452,\n",
       " 'pretended': 15401,\n",
       " 'debts': 14689,\n",
       " '##cakes': 23393,\n",
       " '##wo': 12821,\n",
       " 'abundance': 15569,\n",
       " '##tended': 21857,\n",
       " '##ceptor': 25979,\n",
       " 'July': 1351,\n",
       " '##ented': 22666,\n",
       " 'Shall': 17604,\n",
       " 'separated': 4757,\n",
       " '##CC': 12096,\n",
       " 'Sprint': 15782,\n",
       " 'Salzburg': 19802,\n",
       " 'arrived': 2474,\n",
       " 'residues': 25513,\n",
       " 'activist': 7041,\n",
       " '##nsis': 11261,\n",
       " 'Sakura': 26364,\n",
       " '##nant': 14618,\n",
       " '##rama': 21571,\n",
       " '##link': 13255,\n",
       " 'Basque': 14577,\n",
       " 'trouble': 3819,\n",
       " '##vise': 16641,\n",
       " 'Indeed': 10364,\n",
       " 'thereafter': 7321,\n",
       " '195': 18500,\n",
       " 'Jennifer': 7184,\n",
       " '##ines': 8515,\n",
       " 'Rouge': 13993,\n",
       " 'title': 1641,\n",
       " 'terms': 2538,\n",
       " 'bows': 24179,\n",
       " 'Majesty': 10637,\n",
       " '##tment': 22668,\n",
       " 'Partnership': 17330,\n",
       " 'UHF': 21962,\n",
       " 'hood': 11542,\n",
       " 'Dogs': 16406,\n",
       " 'Miguel': 7975,\n",
       " '##ム': 28858,\n",
       " 'Underworld': 22614,\n",
       " 'arrest': 6040,\n",
       " '##oop': 24064,\n",
       " 'indicate': 5057,\n",
       " 'spread': 2819,\n",
       " 'Healthcare': 22993,\n",
       " 'centre': 2642,\n",
       " 'Ł': 305,\n",
       " 'violence': 4289,\n",
       " '##reased': 20934,\n",
       " '##runch': 22715,\n",
       " '##oker': 26218,\n",
       " 'Estate': 9765,\n",
       " 'Appearance': 27717,\n",
       " 'weighed': 13025,\n",
       " 'Galerie': 19851,\n",
       " '##№': 28735,\n",
       " '##st': 2050,\n",
       " 'hope': 2810,\n",
       " 'narrowly': 11982,\n",
       " 'Alps': 14316,\n",
       " '##postle': 21953,\n",
       " '##ns': 2316,\n",
       " 'shell': 5963,\n",
       " '##arian': 7968,\n",
       " 'few': 1374,\n",
       " 'spared': 18173,\n",
       " 'decisions': 6134,\n",
       " 'coincided': 20126,\n",
       " 'Schumacher': 24656,\n",
       " 'peaks': 13175,\n",
       " 'Sustainable': 21779,\n",
       " 'Jurassic': 21834,\n",
       " '##uing': 21490,\n",
       " '##ries': 3377,\n",
       " '##ps': 3491,\n",
       " 'Bridge': 3640,\n",
       " 'devote': 26344,\n",
       " 'ח': 542,\n",
       " 'δ': 421,\n",
       " 'wanna': 16445,\n",
       " 'Rama': 15078,\n",
       " 'م': 589,\n",
       " '##nney': 26335,\n",
       " 'fragments': 11062,\n",
       " 'tricks': 13270,\n",
       " 'Akron': 25324,\n",
       " 'frozen': 7958,\n",
       " 'doubled': 11590,\n",
       " '##utile': 26555,\n",
       " '##lesh': 27548,\n",
       " 'puppy': 21566,\n",
       " 'CB': 18893,\n",
       " '##mmal': 27568,\n",
       " 'sends': 10130,\n",
       " 'ṃ': 732,\n",
       " 'Ballet': 10773,\n",
       " '##pt': 6451,\n",
       " 'Motorsports': 25683,\n",
       " '##fant': 26636,\n",
       " 'My': 1422,\n",
       " 'Cantonese': 24686,\n",
       " 'emotionally': 15962,\n",
       " '##inted': 18862,\n",
       " 'Jonathan': 4947,\n",
       " 'determine': 4959,\n",
       " 'boarding': 9678,\n",
       " 'deliberately': 9938,\n",
       " 'ῆ': 775,\n",
       " '##russ': 25357,\n",
       " 'Sheila': 15829,\n",
       " '##voking': 20202,\n",
       " '##tip': 27961,\n",
       " '##ジ': 28838,\n",
       " '##spense': 21643,\n",
       " '##lda': 23253,\n",
       " '##hire': 15189,\n",
       " 'crow': 25586,\n",
       " '1898': 5381,\n",
       " '##teca': 27560,\n",
       " 'replay': 18723,\n",
       " 'Bed': 26573,\n",
       " '##uted': 18527,\n",
       " 'crush': 11321,\n",
       " '##its': 6439,\n",
       " '##oshi': 22437,\n",
       " 'backdrop': 20552,\n",
       " 'Lion': 9679,\n",
       " 'pepper': 18700,\n",
       " '##erer': 22113,\n",
       " 'Liz': 8719,\n",
       " 'swords': 12604,\n",
       " 'adding': 5321,\n",
       " 'skull': 7753,\n",
       " '##erated': 20725,\n",
       " 'bravery': 19574,\n",
       " 'Konstantin': 26956,\n",
       " '়': 662,\n",
       " '##rat': 7625,\n",
       " '##nut': 12251,\n",
       " '##rise': 17789,\n",
       " '##anza': 18260,\n",
       " '6th': 4584,\n",
       " 'clergy': 12538,\n",
       " 'Gavin': 9152,\n",
       " 'Since': 1967,\n",
       " 'receiving': 4172,\n",
       " 'Following': 2485,\n",
       " '##ann': 11657,\n",
       " 'Francesco': 11400,\n",
       " '##ehan': 27749,\n",
       " '##ibly': 15298,\n",
       " 'Crossing': 16188,\n",
       " 'lair': 27359,\n",
       " 'euros': 25921,\n",
       " 'stabbed': 13699,\n",
       " 'CS': 24821,\n",
       " '##́': 28310,\n",
       " 'shadow': 6464,\n",
       " 'treats': 20554,\n",
       " '##as': 2225,\n",
       " 'Namibia': 16228,\n",
       " '##υ': 28356,\n",
       " 'deepest': 19327,\n",
       " 'round': 1668,\n",
       " '##set': 9388,\n",
       " '330': 14747,\n",
       " 'zu': 21458,\n",
       " '##state': 19596,\n",
       " '##¡': 28157,\n",
       " '##士': 28901,\n",
       " '##tical': 10165,\n",
       " 'stomped': 24299,\n",
       " 'Nepal': 7795,\n",
       " 'happiness': 9266,\n",
       " '##zal': 25049,\n",
       " '##rmon': 26654,\n",
       " 'brother': 1711,\n",
       " 'Rule': 11575,\n",
       " 'stayed': 3523,\n",
       " 'survives': 14134,\n",
       " '##dox': 15942,\n",
       " 'useless': 12277,\n",
       " 'Ferris': 26428,\n",
       " 'gripping': 15436,\n",
       " 'own': 1319,\n",
       " 'Todd': 6365,\n",
       " 'licenses': 17488,\n",
       " 'possibly': 3566,\n",
       " 'fair': 4652,\n",
       " '##encies': 15672,\n",
       " 'ف': 585,\n",
       " 'tissue': 7918,\n",
       " 'comprises': 8302,\n",
       " 'gallons': 20188,\n",
       " '##uti': 16065,\n",
       " 'ì': 258,\n",
       " 'Babu': 23475,\n",
       " 'selections': 19674,\n",
       " '148': 17474,\n",
       " 'irregular': 12692,\n",
       " 'minded': 13767,\n",
       " 'blank': 9153,\n",
       " '##থ': 28552,\n",
       " 'softly': 4526,\n",
       " 'Fellowship': 9508,\n",
       " 'NFL': 4279,\n",
       " '##μ': 28349,\n",
       " 'Kennedy': 5107,\n",
       " 'explorer': 16171,\n",
       " '##bark': 24063,\n",
       " '##gren': 13421,\n",
       " '##ems': 14587,\n",
       " 'table': 1952,\n",
       " 'rear': 3876,\n",
       " 'Syria': 7303,\n",
       " 'swirling': 17814,\n",
       " 'collaborated': 8303,\n",
       " 'Comet': 28088,\n",
       " 'ᶠ': 725,\n",
       " 'Douglas': 4402,\n",
       " '##ored': 13841,\n",
       " '##xing': 24118,\n",
       " 'resumed': 7475,\n",
       " '##ع': 28490,\n",
       " 'automobile': 11707,\n",
       " '##anga': 21842,\n",
       " '1500': 10204,\n",
       " 'following': 1378,\n",
       " 'loomed': 27401,\n",
       " 'Comic': 14536,\n",
       " '##VC': 19076,\n",
       " 'b': 171,\n",
       " 'cultures': 8708,\n",
       " 'Africans': 19442,\n",
       " 'Siena': 26606,\n",
       " 'View': 10344,\n",
       " 'Coal': 14250,\n",
       " 'owl': 19976,\n",
       " 'opportunities': 6305,\n",
       " 'seniors': 20566,\n",
       " 'Faces': 25246,\n",
       " '##LS': 15928,\n",
       " '##tite': 23723,\n",
       " 'editor': 3045,\n",
       " 'maintenance': 5972,\n",
       " 'Lots': 24270,\n",
       " 'alternatives': 18815,\n",
       " 'third': 1503,\n",
       " 'Dexter': 15815,\n",
       " 'meeting': 2309,\n",
       " 'athlete': 8765,\n",
       " 'Gilmore': 25624,\n",
       " 'Beat': 11938,\n",
       " 'NYC': 17520,\n",
       " 'traders': 14552,\n",
       " 'Digest': 27348,\n",
       " 'Rite': 23787,\n",
       " 'detailed': 6448,\n",
       " '##ination': 9400,\n",
       " '##ygiene': 21431,\n",
       " '##☉': 28769,\n",
       " 'fried': 15688,\n",
       " 'Producer': 8033,\n",
       " '120': 5356,\n",
       " 'breaks': 7610,\n",
       " 'displaced': 13577,\n",
       " 'poisoning': 17539,\n",
       " 'improvised': 22421,\n",
       " 'stationary': 19255,\n",
       " 'employ': 12912,\n",
       " '##iable': 24474,\n",
       " 'Make': 7102,\n",
       " 'alert': 10427,\n",
       " '##lled': 11572,\n",
       " '##plied': 18148,\n",
       " 'Tory': 18377,\n",
       " 'Melbourne': 4141,\n",
       " 'Conversely': 24433,\n",
       " 'unlocked': 14776,\n",
       " '##iano': 16265,\n",
       " 'intermediate': 9533,\n",
       " 'prefers': 21042,\n",
       " 'Musée': 21808,\n",
       " 'residency': 16917,\n",
       " 'intervention': 9108,\n",
       " 'marble': 8004,\n",
       " '##ieu': 21114,\n",
       " 'kinetic': 25433,\n",
       " 'Handbook': 18267,\n",
       " '630': 26384,\n",
       " 'bait': 21012,\n",
       " 'pure': 5805,\n",
       " 'pills': 17029,\n",
       " 'Loch': 18207,\n",
       " 'Côte': 23385,\n",
       " 'war': 1594,\n",
       " 'shot': 2046,\n",
       " 'Wild': 5469,\n",
       " 'cloak': 12923,\n",
       " 'fork': 13097,\n",
       " '##ric': 4907,\n",
       " 'Æ': 231,\n",
       " 'receives': 7881,\n",
       " '##OM': 13041,\n",
       " 'character': 1959,\n",
       " 'Status': 22130,\n",
       " 'ticket': 7260,\n",
       " 'Suzanne': 15915,\n",
       " '##iler': 25614,\n",
       " 'cough': 21810,\n",
       " 'fingertips': 12441,\n",
       " 'dinosaur': 23569,\n",
       " 'Gazette': 11698,\n",
       " '##idas': 23358,\n",
       " '##stituted': 26742,\n",
       " 'Exit': 19588,\n",
       " '##haw': 14431,\n",
       " 'Maker': 23279,\n",
       " 'bucks': 26429,\n",
       " 'malaria': 23645,\n",
       " 'Little': 2743,\n",
       " 'conversations': 12705,\n",
       " 'farmland': 17790,\n",
       " 'rushing': 8248,\n",
       " 'Clint': 17267,\n",
       " '##cede': 19482,\n",
       " 'generated': 6455,\n",
       " '##TR': 23313,\n",
       " 'Flying': 7769,\n",
       " '##eum': 14136,\n",
       " 'Mughal': 19333,\n",
       " '##pard': 18573,\n",
       " 'Ping': 27163,\n",
       " 'commissioner': 12425,\n",
       " '##iselle': 22080,\n",
       " 'outlook': 25059,\n",
       " 'supplying': 19229,\n",
       " '##boa': 22126,\n",
       " 'projecting': 20266,\n",
       " 'bumps': 21090,\n",
       " '##ilize': 21225,\n",
       " 'Psychology': 11695,\n",
       " 'whitish': 17949,\n",
       " 'Resurrection': 26461,\n",
       " '177': 19478,\n",
       " 'Selection': 20045,\n",
       " 'palace': 5717,\n",
       " 'wholesale': 20767,\n",
       " 'bleak': 26481,\n",
       " 'ō': 312,\n",
       " 'behavior': 4658,\n",
       " 'photography': 6427,\n",
       " 'exciting': 11215,\n",
       " '##bina': 23051,\n",
       " 'Hari': 23803,\n",
       " '##22': 20581,\n",
       " 'slid': 4144,\n",
       " 'administrator': 11065,\n",
       " 'boyfriend': 6508,\n",
       " '##城': 28900,\n",
       " '##will': 17274,\n",
       " '##solidate': 28078,\n",
       " 'min': 11241,\n",
       " 'commissioners': 22207,\n",
       " 'Morse': 18582,\n",
       " 'activated': 9618,\n",
       " 'Berg': 16218,\n",
       " '##varo': 25751,\n",
       " 'High': 1693,\n",
       " '##oper': 19807,\n",
       " '##classical': 23974,\n",
       " 'Robbie': 12751,\n",
       " 'Teresa': 12575,\n",
       " 'van': 3498,\n",
       " 'partial': 7597,\n",
       " 'logical': 11730,\n",
       " '1861': 6255,\n",
       " 'readily': 12337,\n",
       " '##ʂ': 28288,\n",
       " 'illegitimate': 19710,\n",
       " 'midfield': 26599,\n",
       " 'downstairs': 10304,\n",
       " 'Wiley': 20516,\n",
       " '##ja': 3174,\n",
       " 'smiled': 2387,\n",
       " 'continuation': 14961,\n",
       " 'forbidden': 12031,\n",
       " 'notified': 21568,\n",
       " 'emergence': 15351,\n",
       " '##uen': 23404,\n",
       " '##bee': 11891,\n",
       " '龸': 1088,\n",
       " '##borne': 16275,\n",
       " '##ulus': 11430,\n",
       " 'booked': 18951,\n",
       " 'warrior': 8229,\n",
       " 'inmates': 13944,\n",
       " 'McCain': 20884,\n",
       " 'STS': 25435,\n",
       " 'mid': 2286,\n",
       " '##ayne': 24562,\n",
       " 'flexed': 27292,\n",
       " 'naive': 22607,\n",
       " 'Rights': 5399,\n",
       " 'cried': 6104,\n",
       " 'taught': 3188,\n",
       " '##feng': 27875,\n",
       " 'requirement': 8875,\n",
       " 'thankful': 21602,\n",
       " '##ₘ': 28726,\n",
       " '##wan': 5491,\n",
       " 'kicker': 27219,\n",
       " '##mart': 22736,\n",
       " 'alternating': 16307,\n",
       " '##chemist': 27239,\n",
       " 'Lord': 2188,\n",
       " 'ice': 2854,\n",
       " 'awkwardly': 20714,\n",
       " '##]': 28149,\n",
       " 'conference': 3511,\n",
       " '##ₖ': 28725,\n",
       " '##cious': 9589,\n",
       " 'considerations': 19069,\n",
       " 'adaptation': 6350,\n",
       " '##dies': 10831,\n",
       " '##plane': 10648,\n",
       " '##arra': 25203,\n",
       " 'Potter': 11434,\n",
       " 'Idol': 15632,\n",
       " 'zone': 4834,\n",
       " 'strings': 8409,\n",
       " 'Type': 6902,\n",
       " 'NPR': 23706,\n",
       " 'Solid': 20375,\n",
       " 'Montana': 7976,\n",
       " 'Suddenly': 7935,\n",
       " 'benefit': 5257,\n",
       " 'steered': 26386,\n",
       " '##ean': 7766,\n",
       " '##oan': 23516,\n",
       " 'tournament': 2348,\n",
       " 'Zionist': 23726,\n",
       " 'tsunami': 24212,\n",
       " 'area': 1298,\n",
       " 'Jean': 2893,\n",
       " 'members': 1484,\n",
       " 'Clube': 28070,\n",
       " 'Change': 9091,\n",
       " 'Turks': 13126,\n",
       " 'dripped': 25188,\n",
       " 'Extension': 20838,\n",
       " 'feeds': 15044,\n",
       " 'around': 1213,\n",
       " 'Curtis': 8862,\n",
       " 'Jerusalem': 6167,\n",
       " 'TD': 15439,\n",
       " 'Charlton': 19155,\n",
       " '153': 17777,\n",
       " 'Blade': 17360,\n",
       " 'Purdue': 21537,\n",
       " '##iser': 12943,\n",
       " '##top': 9870,\n",
       " '##chs': 17704,\n",
       " 'vertical': 7391,\n",
       " 'Brittany': 12875,\n",
       " 'Marcel': 13737,\n",
       " 'drought': 16076,\n",
       " 'streamed': 20273,\n",
       " '68': 5599,\n",
       " 'ह': 627,\n",
       " 'fourteenth': 20872,\n",
       " 'Ponce': 23349,\n",
       " 'Mean': 25030,\n",
       " 'Babylon': 19014,\n",
       " 'prevailing': 21301,\n",
       " '##omo': 18445,\n",
       " 'І': 446,\n",
       " '##ico': 10658,\n",
       " '女': 1013,\n",
       " 'mud': 9052,\n",
       " '##key': 9144,\n",
       " '##ssed': 17856,\n",
       " '##truct': 23055,\n",
       " '##fty': 27944,\n",
       " '##²': 10731,\n",
       " 'Ships': 21016,\n",
       " '##lessly': 8709,\n",
       " 'outdoors': 23178,\n",
       " 'exercise': 6730,\n",
       " '##島': 28915,\n",
       " 'reasonable': 9483,\n",
       " 'mutual': 9175,\n",
       " 'delicious': 13108,\n",
       " 'Dale': 8238,\n",
       " 'Nikolai': 14374,\n",
       " '##lex': 21729,\n",
       " 'organisations': 8485,\n",
       " 'coughing': 24992,\n",
       " '##dition': 14669,\n",
       " 'Era': 14903,\n",
       " 'Annabelle': 28016,\n",
       " 'links': 6743,\n",
       " 'patrons': 14645,\n",
       " 'earliest': 5041,\n",
       " 'staying': 6218,\n",
       " 'Tudor': 16528,\n",
       " 'Lila': 21384,\n",
       " 'termination': 20776,\n",
       " 'invaders': 22864,\n",
       " 'cock': 11012,\n",
       " 'undertake': 17778,\n",
       " 'insisting': 25504,\n",
       " 'Jett': 25653,\n",
       " '##ễ': 28650,\n",
       " '##lance': 13831,\n",
       " '##kaya': 23898,\n",
       " 'urging': 14992,\n",
       " '##dell': 12065,\n",
       " 'Charley': 22458,\n",
       " '##ggio': 21541,\n",
       " 'Delta': 7679,\n",
       " 'manufactured': 7227,\n",
       " 'violent': 5973,\n",
       " 'Sorry': 6502,\n",
       " 'Venice': 7433,\n",
       " 'absurd': 21321,\n",
       " 'flinched': 20888,\n",
       " 'Security': 4354,\n",
       " 'Piece': 27916,\n",
       " '##icidal': 27804,\n",
       " 'wrecked': 20580,\n",
       " 'Einstein': 16127,\n",
       " 'â': 248,\n",
       " 'casually': 13725,\n",
       " 'precursor': 15985,\n",
       " '##plified': 18580,\n",
       " 'finest': 10812,\n",
       " 'Łódź': 19230,\n",
       " 'Circuit': 7887,\n",
       " 'exhibiting': 25988,\n",
       " 'mushroom': 25590,\n",
       " 'murmured': 6574,\n",
       " '##र': 28524,\n",
       " 'plot': 4928,\n",
       " '##qua': 13284,\n",
       " 'њ': 511,\n",
       " '##aran': 27177,\n",
       " 'Agent': 9341,\n",
       " 'trenches': 22547,\n",
       " '##llie': 14367,\n",
       " 'catch': 3963,\n",
       " '##en': 1424,\n",
       " 'alleged': 6351,\n",
       " '##ligent': 27518,\n",
       " 'Franciscan': 21183,\n",
       " '##ゆ': 28817,\n",
       " '##DC': 15556,\n",
       " 'wished': 5608,\n",
       " 'formally': 5708,\n",
       " '##urage': 26800,\n",
       " 'Minority': 26495,\n",
       " 'alto': 17138,\n",
       " 'dialect': 9222,\n",
       " 'Hale': 13684,\n",
       " '##wal': 11487,\n",
       " 'approximately': 2324,\n",
       " 'Marathi': 19963,\n",
       " '版': 1053,\n",
       " '##icker': 23666,\n",
       " 'acquaintance': 20125,\n",
       " 'creator': 9264,\n",
       " 'style': 1947,\n",
       " 'Rivera': 14556,\n",
       " '##ahan': 14063,\n",
       " 'fragment': 17906,\n",
       " 'SR': 5833,\n",
       " 'ace': 20839,\n",
       " 'Present': 13653,\n",
       " 'commuter': 17397,\n",
       " 'stakeholders': 26027,\n",
       " 'verify': 23073,\n",
       " 'businessman': 6414,\n",
       " 'Middlesex': 14040,\n",
       " 'phrase': 7224,\n",
       " '##lux': 24796,\n",
       " 'whose': 2133,\n",
       " 'montane': 24831,\n",
       " 'phylogenetic': 28084,\n",
       " 'introduce': 8698,\n",
       " 'electrons': 16159,\n",
       " 'Patent': 16653,\n",
       " 'throbbing': 18767,\n",
       " 'suspiciously': 23837,\n",
       " '##Ј': 28365,\n",
       " 'infection': 8974,\n",
       " 'Wave': 13531,\n",
       " 'legitimate': 11582,\n",
       " 'groundbreaking': 27983,\n",
       " 'reject': 16589,\n",
       " 'dramatic': 7271,\n",
       " 'parachute': 22021,\n",
       " '##skin': 22934,\n",
       " 'destroyers': 14821,\n",
       " '##van': 5242,\n",
       " 'Ceylon': 17469,\n",
       " 'polynomial': 19068,\n",
       " '##ocks': 18347,\n",
       " 'bowler': 17124,\n",
       " '1847': 8841,\n",
       " 'Sweeney': 23306,\n",
       " 'taxa': 27522,\n",
       " '##ַ': 28441,\n",
       " 'Bring': 15646,\n",
       " 'Liberal': 4561,\n",
       " '##ph': 7880,\n",
       " 'sack': 14204,\n",
       " '##dock': 16417,\n",
       " 'Then': 1599,\n",
       " 'Sears': 20590,\n",
       " '##sp': 20080,\n",
       " 'um': 15276,\n",
       " 'shudder': 19986,\n",
       " 'Reacher': 25838,\n",
       " 'view': 2458,\n",
       " 'bakery': 23986,\n",
       " 'vague': 14673,\n",
       " 'cheeks': 5388,\n",
       " '##tree': 14750,\n",
       " 'worried': 4472,\n",
       " 'Gary': 4926,\n",
       " 'higher': 2299,\n",
       " 'Demon': 16805,\n",
       " '##ères': 16981,\n",
       " 'absence': 5884,\n",
       " 'certified': 7720,\n",
       " 'Rossi': 20154,\n",
       " 'List': 5619,\n",
       " '##₉': 28716,\n",
       " 'woman': 1590,\n",
       " '##tropical': 27144,\n",
       " 'magical': 9214,\n",
       " '##held': 17674,\n",
       " 'Sand': 16377,\n",
       " 'covert': 25015,\n",
       " 'columnist': 14219,\n",
       " 'Hybrid': 27602,\n",
       " 'fridge': 18243,\n",
       " 'grateful': 9473,\n",
       " 'monkey': 16019,\n",
       " 'occupy': 12774,\n",
       " 'Brotherhood': 15831,\n",
       " 'everyone': 2490,\n",
       " 'civilian': 6688,\n",
       " '##minster': 27057,\n",
       " '[unused65]': 65,\n",
       " 'Federation': 4245,\n",
       " '##quent': 14855,\n",
       " 'Buchanan': 14912,\n",
       " '##ₑ': 28721,\n",
       " '##ting': 1916,\n",
       " 'clocks': 24998,\n",
       " '##age': 2553,\n",
       " 'reviewer': 12827,\n",
       " 'Hamlet': 20332,\n",
       " 'Style': 13023,\n",
       " 'ö': 268,\n",
       " 'absolute': 7846,\n",
       " '##ל': 28456,\n",
       " '530': 26260,\n",
       " '##EL': 21678,\n",
       " '##ungen': 25821,\n",
       " 'Lisbon': 12007,\n",
       " 'demon': 5725,\n",
       " '‚': 788,\n",
       " 'golf': 7135,\n",
       " '##ort': 12148,\n",
       " 'smack': 25767,\n",
       " 'sold': 1962,\n",
       " 'convictions': 22978,\n",
       " 'Cadet': 20306,\n",
       " 'Cobb': 18358,\n",
       " 'Parkinson': 22195,\n",
       " '##posed': 13541,\n",
       " 'highly': 3023,\n",
       " '##ión': 11376,\n",
       " '##lius': 14968,\n",
       " '##Ч': 28388,\n",
       " 'Hitler': 7579,\n",
       " 'housekeeper': 26458,\n",
       " 'bolted': 19532,\n",
       " 'cruelty': 22125,\n",
       " '##cluded': 23498,\n",
       " '##lio': 9436,\n",
       " 'Picture': 10041,\n",
       " 'HBO': 15262,\n",
       " 'Moss': 13626,\n",
       " 'pumped': 17967,\n",
       " 'Maynard': 24518,\n",
       " 'spotted': 6910,\n",
       " 'López': 13629,\n",
       " 'si': 27466,\n",
       " 'analytical': 22828,\n",
       " 'centered': 8663,\n",
       " 'terminus': 7132,\n",
       " '##lement': 20041,\n",
       " '##jong': 25507,\n",
       " 'Doris': 16434,\n",
       " 'Peacock': 26102,\n",
       " '小': 1019,\n",
       " 'Harrisburg': 27676,\n",
       " 'wartime': 13425,\n",
       " 'expression': 2838,\n",
       " 'Exempt': 19861,\n",
       " '[unused92]': 92,\n",
       " 'Galaxy': 13021,\n",
       " '##ppan': 28087,\n",
       " '##UE': 24846,\n",
       " 'workplace': 19328,\n",
       " 'shuffled': 20390,\n",
       " 'Arabia': 8945,\n",
       " 'decent': 11858,\n",
       " 'Disease': 20012,\n",
       " 'backseat': 22375,\n",
       " '[unused62]': 62,\n",
       " 'stellar': 21371,\n",
       " 'for': 1111,\n",
       " 'Orchestra': 4590,\n",
       " '∈': 850,\n",
       " 'heirs': 17461,\n",
       " '1960s': 3266,\n",
       " 'conceal': 21689,\n",
       " 'Radio': 2664,\n",
       " 'search': 3403,\n",
       " 'alright': 15354,\n",
       " 'king': 2226,\n",
       " 'generations': 8225,\n",
       " '##aid': 19954,\n",
       " 'reversed': 11802,\n",
       " 'Pentagon': 25122,\n",
       " 'Hamburg': 8339,\n",
       " 'Théâtre': 18519,\n",
       " 'Phelps': 22385,\n",
       " 'control': 1654,\n",
       " 'terrorism': 12010,\n",
       " 'Clock': 20957,\n",
       " 'visions': 15219,\n",
       " 'hazard': 22699,\n",
       " 'Cascade': 27173,\n",
       " '##horse': 16336,\n",
       " '##nking': 20698,\n",
       " 'Yale': 7534,\n",
       " '215': 18615,\n",
       " 'slowing': 20098,\n",
       " 'attendance': 7794,\n",
       " '##HF': 13561,\n",
       " 'spare': 8608,\n",
       " '##ulse': 20996,\n",
       " 'cruiser': 12680,\n",
       " '##uxe': 26731,\n",
       " 'Ultimately': 16266,\n",
       " 'bursts': 21254,\n",
       " 'down': 1205,\n",
       " '##taire': 25482,\n",
       " '##:': 28140,\n",
       " 'sense': 2305,\n",
       " 'Rainbow': 13188,\n",
       " 'Militia': 18549,\n",
       " '王': 1055,\n",
       " 'submarine': 7027,\n",
       " 'directional': 23317,\n",
       " 'stack': 10926,\n",
       " 'Elias': 16325,\n",
       " 'popularity': 5587,\n",
       " 'ু': 666,\n",
       " 'Finance': 7476,\n",
       " 'exaggerated': 18088,\n",
       " 'eligible': 7408,\n",
       " '##も': 28815,\n",
       " 'invaded': 10784,\n",
       " 'than': 1190,\n",
       " '##SF': 19261,\n",
       " 'Movement': 6257,\n",
       " '1817': 12698,\n",
       " 'vow': 22417,\n",
       " 'supplemented': 22625,\n",
       " 'interstate': 20905,\n",
       " 'photographic': 15368,\n",
       " '##ark': 23822,\n",
       " '##bbe': 20584,\n",
       " ...}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"What was that? a hard question\"\n",
    "q = \"What was that?\"\n",
    "c = \"a hard question\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS] Nơi nào [UNK] trên kênh Saint - Martin nối liền bồn Villette và thượng lưu sông Seine? [SEP] Dòng sông Seine chảy qua Paris theo hình một cánh cung : vào thành phố từ phía đông nam và ra [UNK] thành phố phía tây bắc. Hơn 30 cây cầu của Paris bắc qua dòng sông này. Còn có hai dòng chảy khác qua Paris. Sông Bièvre phía nam, ngày nay ngầm hoàn toàn dưới đất. Kênh Saint - Martin hoàn thành 1825 dài 4, 5 km, nối với bồn Villette và vào thành phố [UNK] phía đông bắc. Kênh Saint - Martin chảy ngầm dưới đất cho tới phố Faubourg - du - Temple [UNK] quảng trường Bastille rồi tiếp tục chảy lộ thiên và nối với sông Seine [UNK] phía thượng lưu của đảo Île Saint - Louis. Một con kênh khác là Saint - Denis, cũng nối với bồn Villette theo hướng Saint - Denis, dài 4, 5 km và hoàn thành vào năm 1821. Con kênh này [UNK] sông Seine [UNK] phần hạ lưu và không đi qua Paris. Một dòng sông nữa là Marne, chảy gần Paris qua Seine - Saint - Denis, Val - de - Marne và [UNK] sông Seine [UNK] phía đông nam thành phố. [SEP]',\n",
       " [(0, 0),\n",
       "  (0, 1),\n",
       "  (1, 2),\n",
       "  (2, 3),\n",
       "  (4, 5),\n",
       "  (5, 6),\n",
       "  (6, 7),\n",
       "  (8, 11),\n",
       "  (12, 13),\n",
       "  (13, 14),\n",
       "  (14, 15),\n",
       "  (15, 16),\n",
       "  (17, 18),\n",
       "  (18, 19),\n",
       "  (19, 21),\n",
       "  (22, 27),\n",
       "  (27, 28),\n",
       "  (28, 34),\n",
       "  (35, 36),\n",
       "  (36, 37),\n",
       "  (37, 38),\n",
       "  (39, 40),\n",
       "  (40, 41),\n",
       "  (41, 42),\n",
       "  (42, 43),\n",
       "  (44, 45),\n",
       "  (45, 46),\n",
       "  (46, 47),\n",
       "  (48, 53),\n",
       "  (53, 56),\n",
       "  (57, 58),\n",
       "  (58, 59),\n",
       "  (60, 62),\n",
       "  (62, 63),\n",
       "  (63, 64),\n",
       "  (64, 66),\n",
       "  (67, 68),\n",
       "  (68, 69),\n",
       "  (69, 70),\n",
       "  (71, 72),\n",
       "  (72, 73),\n",
       "  (73, 75),\n",
       "  (76, 81),\n",
       "  (81, 82),\n",
       "  (0, 0),\n",
       "  (0, 1),\n",
       "  (1, 2),\n",
       "  (2, 4),\n",
       "  (5, 6),\n",
       "  (6, 7),\n",
       "  (7, 9),\n",
       "  (10, 15),\n",
       "  (16, 18),\n",
       "  (18, 19),\n",
       "  (19, 20),\n",
       "  (21, 22),\n",
       "  (22, 24),\n",
       "  (25, 30),\n",
       "  (31, 34),\n",
       "  (34, 35),\n",
       "  (36, 37),\n",
       "  (37, 38),\n",
       "  (38, 40),\n",
       "  (41, 42),\n",
       "  (42, 43),\n",
       "  (43, 44),\n",
       "  (45, 46),\n",
       "  (46, 48),\n",
       "  (48, 49),\n",
       "  (50, 52),\n",
       "  (52, 54),\n",
       "  (54, 55),\n",
       "  (56, 57),\n",
       "  (57, 58),\n",
       "  (58, 59),\n",
       "  (60, 62),\n",
       "  (62, 63),\n",
       "  (63, 65),\n",
       "  (66, 67),\n",
       "  (67, 68),\n",
       "  (68, 69),\n",
       "  (70, 71),\n",
       "  (71, 72),\n",
       "  (73, 74),\n",
       "  (74, 75),\n",
       "  (75, 77),\n",
       "  (78, 79),\n",
       "  (79, 80),\n",
       "  (80, 82),\n",
       "  (83, 85),\n",
       "  (85, 86),\n",
       "  (87, 88),\n",
       "  (88, 89),\n",
       "  (90, 91),\n",
       "  (91, 92),\n",
       "  (93, 97),\n",
       "  (98, 100),\n",
       "  (100, 101),\n",
       "  (101, 103),\n",
       "  (104, 105),\n",
       "  (105, 106),\n",
       "  (106, 107),\n",
       "  (108, 109),\n",
       "  (109, 110),\n",
       "  (110, 112),\n",
       "  (113, 114),\n",
       "  (114, 115),\n",
       "  (115, 116),\n",
       "  (117, 118),\n",
       "  (118, 119),\n",
       "  (119, 120),\n",
       "  (120, 121),\n",
       "  (122, 123),\n",
       "  (123, 124),\n",
       "  (124, 125),\n",
       "  (126, 128),\n",
       "  (129, 130),\n",
       "  (130, 131),\n",
       "  (131, 132),\n",
       "  (133, 134),\n",
       "  (134, 135),\n",
       "  (135, 136),\n",
       "  (137, 138),\n",
       "  (138, 139),\n",
       "  (139, 140),\n",
       "  (141, 146),\n",
       "  (147, 148),\n",
       "  (148, 149),\n",
       "  (149, 150),\n",
       "  (151, 152),\n",
       "  (152, 154),\n",
       "  (155, 156),\n",
       "  (156, 157),\n",
       "  (157, 159),\n",
       "  (160, 161),\n",
       "  (161, 162),\n",
       "  (162, 164),\n",
       "  (165, 166),\n",
       "  (166, 167),\n",
       "  (167, 168),\n",
       "  (168, 169),\n",
       "  (170, 171),\n",
       "  (171, 172),\n",
       "  (172, 173),\n",
       "  (174, 175),\n",
       "  (175, 176),\n",
       "  (177, 179),\n",
       "  (179, 180),\n",
       "  (181, 182),\n",
       "  (182, 183),\n",
       "  (183, 185),\n",
       "  (186, 188),\n",
       "  (188, 189),\n",
       "  (189, 190),\n",
       "  (191, 192),\n",
       "  (192, 193),\n",
       "  (193, 194),\n",
       "  (194, 195),\n",
       "  (196, 197),\n",
       "  (197, 199),\n",
       "  (200, 205),\n",
       "  (205, 206),\n",
       "  (207, 208),\n",
       "  (208, 209),\n",
       "  (209, 211),\n",
       "  (212, 213),\n",
       "  (213, 214),\n",
       "  (214, 215),\n",
       "  (215, 218),\n",
       "  (219, 220),\n",
       "  (220, 221),\n",
       "  (221, 223),\n",
       "  (224, 226),\n",
       "  (226, 227),\n",
       "  (227, 228),\n",
       "  (229, 231),\n",
       "  (231, 232),\n",
       "  (232, 233),\n",
       "  (234, 236),\n",
       "  (236, 237),\n",
       "  (238, 240),\n",
       "  (240, 241),\n",
       "  (241, 242),\n",
       "  (243, 245),\n",
       "  (245, 246),\n",
       "  (246, 247),\n",
       "  (248, 250),\n",
       "  (250, 251),\n",
       "  (251, 252),\n",
       "  (253, 254),\n",
       "  (254, 255),\n",
       "  (255, 256),\n",
       "  (256, 257),\n",
       "  (258, 259),\n",
       "  (259, 260),\n",
       "  (260, 261),\n",
       "  (261, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 267),\n",
       "  (268, 273),\n",
       "  (273, 274),\n",
       "  (274, 280),\n",
       "  (281, 283),\n",
       "  (283, 284),\n",
       "  (284, 285),\n",
       "  (286, 288),\n",
       "  (288, 289),\n",
       "  (289, 291),\n",
       "  (292, 296),\n",
       "  (297, 298),\n",
       "  (298, 299),\n",
       "  (299, 300),\n",
       "  (301, 302),\n",
       "  (302, 303),\n",
       "  (303, 304),\n",
       "  (305, 307),\n",
       "  (307, 308),\n",
       "  (309, 310),\n",
       "  (310, 311),\n",
       "  (311, 312),\n",
       "  (313, 314),\n",
       "  (314, 315),\n",
       "  (315, 316),\n",
       "  (317, 318),\n",
       "  (318, 319),\n",
       "  (319, 320),\n",
       "  (321, 326),\n",
       "  (326, 329),\n",
       "  (330, 331),\n",
       "  (331, 332),\n",
       "  (333, 334),\n",
       "  (334, 335),\n",
       "  (335, 336),\n",
       "  (337, 339),\n",
       "  (339, 340),\n",
       "  (340, 342),\n",
       "  (343, 344),\n",
       "  (344, 345),\n",
       "  (345, 346),\n",
       "  (347, 348),\n",
       "  (349, 350),\n",
       "  (350, 351),\n",
       "  (351, 353),\n",
       "  (354, 355),\n",
       "  (355, 356),\n",
       "  (356, 358),\n",
       "  (359, 360),\n",
       "  (360, 361),\n",
       "  (361, 362),\n",
       "  (362, 363),\n",
       "  (364, 365),\n",
       "  (365, 366),\n",
       "  (366, 368),\n",
       "  (369, 374),\n",
       "  (374, 375),\n",
       "  (375, 381),\n",
       "  (382, 384),\n",
       "  (384, 385),\n",
       "  (385, 386),\n",
       "  (387, 389),\n",
       "  (389, 390),\n",
       "  (390, 391),\n",
       "  (392, 393),\n",
       "  (393, 394),\n",
       "  (394, 395),\n",
       "  (395, 396),\n",
       "  (397, 398),\n",
       "  (398, 399),\n",
       "  (399, 400),\n",
       "  (401, 403),\n",
       "  (403, 404),\n",
       "  (405, 406),\n",
       "  (406, 407),\n",
       "  (407, 408),\n",
       "  (409, 410),\n",
       "  (410, 411),\n",
       "  (411, 412),\n",
       "  (413, 414),\n",
       "  (414, 416),\n",
       "  (416, 421),\n",
       "  (421, 422),\n",
       "  (422, 424),\n",
       "  (424, 425),\n",
       "  (425, 431),\n",
       "  (432, 433),\n",
       "  (434, 435),\n",
       "  (435, 436),\n",
       "  (436, 437),\n",
       "  (437, 439),\n",
       "  (440, 441),\n",
       "  (441, 442),\n",
       "  (442, 443),\n",
       "  (443, 444),\n",
       "  (444, 446),\n",
       "  (447, 449),\n",
       "  (449, 451),\n",
       "  (451, 455),\n",
       "  (456, 457),\n",
       "  (457, 458),\n",
       "  (458, 459),\n",
       "  (460, 461),\n",
       "  (461, 462),\n",
       "  (462, 463),\n",
       "  (463, 464),\n",
       "  (465, 466),\n",
       "  (466, 467),\n",
       "  (467, 468),\n",
       "  (469, 471),\n",
       "  (471, 472),\n",
       "  (472, 473),\n",
       "  (474, 475),\n",
       "  (475, 476),\n",
       "  (477, 479),\n",
       "  (479, 480),\n",
       "  (480, 481),\n",
       "  (481, 482),\n",
       "  (483, 484),\n",
       "  (484, 485),\n",
       "  (486, 487),\n",
       "  (487, 488),\n",
       "  (488, 489),\n",
       "  (490, 491),\n",
       "  (491, 492),\n",
       "  (492, 493),\n",
       "  (494, 495),\n",
       "  (495, 496),\n",
       "  (496, 498),\n",
       "  (499, 504),\n",
       "  (505, 506),\n",
       "  (507, 508),\n",
       "  (508, 509),\n",
       "  (509, 511),\n",
       "  (512, 514),\n",
       "  (514, 515),\n",
       "  (515, 516),\n",
       "  (516, 518),\n",
       "  (519, 520),\n",
       "  (520, 521),\n",
       "  (521, 522),\n",
       "  (523, 524),\n",
       "  (524, 525),\n",
       "  (525, 526),\n",
       "  (527, 528),\n",
       "  (528, 529),\n",
       "  (529, 530),\n",
       "  (531, 534),\n",
       "  (535, 540),\n",
       "  (540, 541),\n",
       "  (541, 546),\n",
       "  (546, 547),\n",
       "  (548, 549),\n",
       "  (549, 550),\n",
       "  (550, 551),\n",
       "  (552, 555),\n",
       "  (556, 557),\n",
       "  (557, 558),\n",
       "  (558, 560),\n",
       "  (561, 562),\n",
       "  (562, 563),\n",
       "  (563, 564),\n",
       "  (564, 565),\n",
       "  (566, 567),\n",
       "  (567, 568),\n",
       "  (569, 574),\n",
       "  (574, 575),\n",
       "  (575, 580),\n",
       "  (580, 581),\n",
       "  (582, 583),\n",
       "  (583, 584),\n",
       "  (584, 586),\n",
       "  (587, 588),\n",
       "  (588, 589),\n",
       "  (589, 590),\n",
       "  (591, 592),\n",
       "  (592, 593),\n",
       "  (593, 594),\n",
       "  (595, 596),\n",
       "  (596, 597),\n",
       "  (597, 598),\n",
       "  (599, 604),\n",
       "  (604, 607),\n",
       "  (608, 611),\n",
       "  (611, 612),\n",
       "  (613, 614),\n",
       "  (614, 615),\n",
       "  (615, 616),\n",
       "  (616, 618),\n",
       "  (619, 624),\n",
       "  (624, 625),\n",
       "  (625, 630),\n",
       "  (630, 631),\n",
       "  (632, 633),\n",
       "  (633, 634),\n",
       "  (634, 635),\n",
       "  (636, 637),\n",
       "  (637, 638),\n",
       "  (638, 639),\n",
       "  (640, 642),\n",
       "  (643, 644),\n",
       "  (644, 645),\n",
       "  (646, 648),\n",
       "  (648, 649),\n",
       "  (649, 650),\n",
       "  (651, 653),\n",
       "  (653, 654),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (658, 659),\n",
       "  (659, 660),\n",
       "  (661, 662),\n",
       "  (662, 663),\n",
       "  (663, 664),\n",
       "  (665, 669),\n",
       "  (669, 670),\n",
       "  (671, 674),\n",
       "  (675, 676),\n",
       "  (676, 677),\n",
       "  (677, 679),\n",
       "  (680, 681),\n",
       "  (681, 682),\n",
       "  (682, 683),\n",
       "  (684, 687),\n",
       "  (688, 689),\n",
       "  (689, 690),\n",
       "  (690, 692),\n",
       "  (693, 698),\n",
       "  (699, 700),\n",
       "  (701, 702),\n",
       "  (702, 703),\n",
       "  (703, 704),\n",
       "  (704, 705),\n",
       "  (706, 707),\n",
       "  (707, 708),\n",
       "  (709, 710),\n",
       "  (710, 711),\n",
       "  (711, 712),\n",
       "  (713, 714),\n",
       "  (714, 715),\n",
       "  (716, 717),\n",
       "  (717, 718),\n",
       "  (718, 719),\n",
       "  (719, 721),\n",
       "  (722, 723),\n",
       "  (723, 724),\n",
       "  (725, 726),\n",
       "  (726, 728),\n",
       "  (729, 734),\n",
       "  (734, 735),\n",
       "  (736, 737),\n",
       "  (737, 738),\n",
       "  (738, 739),\n",
       "  (740, 741),\n",
       "  (741, 742),\n",
       "  (742, 744),\n",
       "  (745, 746),\n",
       "  (746, 747),\n",
       "  (747, 749),\n",
       "  (750, 751),\n",
       "  (751, 752),\n",
       "  (752, 753),\n",
       "  (754, 755),\n",
       "  (755, 756),\n",
       "  (757, 760),\n",
       "  (760, 762),\n",
       "  (762, 763),\n",
       "  (764, 766),\n",
       "  (766, 767),\n",
       "  (767, 768),\n",
       "  (769, 770),\n",
       "  (770, 771),\n",
       "  (771, 772),\n",
       "  (773, 778),\n",
       "  (779, 780),\n",
       "  (780, 782),\n",
       "  (783, 788),\n",
       "  (788, 789),\n",
       "  (789, 794),\n",
       "  (794, 795),\n",
       "  (795, 800),\n",
       "  (800, 801),\n",
       "  (802, 805),\n",
       "  (805, 806),\n",
       "  (806, 808),\n",
       "  (808, 809),\n",
       "  (809, 812),\n",
       "  (812, 814),\n",
       "  (815, 816),\n",
       "  (816, 817),\n",
       "  (818, 821),\n",
       "  (822, 823),\n",
       "  (823, 824),\n",
       "  (824, 826),\n",
       "  (827, 832),\n",
       "  (833, 834),\n",
       "  (835, 836),\n",
       "  (836, 837),\n",
       "  (837, 839),\n",
       "  (840, 841),\n",
       "  (841, 842),\n",
       "  (842, 844),\n",
       "  (845, 847),\n",
       "  (847, 848),\n",
       "  (849, 851),\n",
       "  (851, 852),\n",
       "  (852, 854),\n",
       "  (855, 856),\n",
       "  (856, 857),\n",
       "  (857, 858),\n",
       "  (858, 859),\n",
       "  (0, 0)])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(q, c).input_ids), tokenizer(q, c, return_offsets_mapping=True).offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA_metrics(start, end, start_idx, end_idx, input_ids, tokenizer):\n",
    "    '''\n",
    "    EM and F1 score for text output\n",
    "    start = b x n\n",
    "    '''\n",
    "    EM = 0\n",
    "    F1 = 0\n",
    "    for i in range(start.shape[0]):\n",
    "        pred = tokenizer.decode(input_ids[i][start[i]:end[i]+1])\n",
    "        trues = []\n",
    "        for j in range(len(start_idx[i])):\n",
    "            trues.append(tokenizer.decode(input_ids[i][start_idx[i][j]:end_idx[i][j]+1]))\n",
    "        #exact match\n",
    "        if pred in trues:\n",
    "            EM += 1 \n",
    "       \n",
    "        #F1 score\n",
    "        F1_score = []\n",
    "        for true in trues:\n",
    "            sum = 0\n",
    "            \n",
    "            text = pred if len(pred.split()) < len(true.split()) else true\n",
    "            for i in range(len(text.split())):\n",
    "                if pred.split()[i] == true.split()[i]:\n",
    "                    sum += 1 \n",
    "            if len(pred.split()) == 0 or len(true.split()) == 0:\n",
    "                F1_score.append(int(pred == true))\n",
    "                continue\n",
    "            precision = sum/len(pred.split())\n",
    "            recall = sum/len(true.split())\n",
    "            if precision == 0 or recall == 0:\n",
    "                F1_score.append(0)\n",
    "                continue\n",
    "            \n",
    "            F1_score.append(2/(1/precision + 1/recall))\n",
    "        \n",
    "        F1 += max(F1_score)\n",
    "    return EM/start.shape[0], F1/start.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73fdcbcaa6b22d852c0f9bd9783ab6b1b1c25c52a0a8da76beae07513436cb85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
