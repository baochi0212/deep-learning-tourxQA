{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(32, 8)\n",
    "b = torch.zeros([32, 8], dtype=torch.float)\n",
    "b[0, 0] = 1\n",
    "loss = nn.BCEWithLogitsLoss(reduction='none')(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.where(b != 0, loss, torch.tensor([0.]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.where(b != 0, loss, loss*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 5  # number of tags is 5\n",
    "model = CRF(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "emissions = torch.randn(seq_length, batch_size, num_tags) #n_seq x batch x logits\n",
    "tags = torch.tensor([\n",
    "  [0, 1], [2, 4], [3, 1]\n",
    "], dtype=torch.long)  #nseq x batch\n",
    "loss = model(emissions, tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(30, 32, 142)\n",
    "b = torch.zeros(30, 32, dtype=torch.long)\n",
    "model = CRF(a.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4717.1206, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "name = ['train', 'test', 'dev']\n",
    "for i in name:\n",
    "    for label in open(\"/home/xps/educate/code/hust/XQA/docs/src/JointIDSF/PhoATIS/word-level\" + f'/{i}/label'):\n",
    "        if label.strip() not in list:\n",
    "            list.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_train.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_flight_time', 'atis_airfare', 'atis_aircraft',\n",
       "        'atis_ground_service', 'atis_airline', 'atis_abbreviation',\n",
       "        'atis_quantity'], dtype=object),\n",
       " 4834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'vietanh': 'ngu'}\n",
    "a.update({'vie anh': 'ocho'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_airfare', 'atis_ground_service',\n",
       "        'atis_airline', 'atis_flight_time', 'atis_quantity',\n",
       "        'atis_abbreviation', 'atis_aircraft'], dtype=object),\n",
       " 800)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_test.csv\", header=None)\n",
    "test[0].unique(), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tôi muốn một chuyến bay khứ_hồi từ đồng_hới đến côn_đảo khởi_hành vào thứ tư ngày 16 tháng 6 đến côn_đảo vào buổi tối khoảng 7 giờ và trở về đồng_hới vào buổi tối ngày hôm sau khoảng 7 giờ hãng hàng_không nào có chuyến bay phù_hợp với tuyến đường đó\n"
     ]
    }
   ],
   "source": [
    "lengths = {}\n",
    "with open(\"/home/xps/educate/code/hust/XQA/data/raw/PhoATIS/word-level/train/seq.in\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line.strip().split(' ')) not in lengths.keys():\n",
    "            lengths[len(line.strip().split(' '))] = 1\n",
    "        else:\n",
    "            lengths[len(line.strip().split(' '))] += 1 \n",
    "        if len(line.strip().split(' ')) == 50:\n",
    "            print(line.strip()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lengths, key = lambda d: lengths[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoctro'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename('/hoa/hoctro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "os.environ['dir'] = \"/home/xps/educate/code/hust/XQA/data\"\n",
    "from utils.preprocess import string2list, get_label\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "data_dir = os.environ['dir']\n",
    "raw_dir = data_dir + '/data/raw/PhoATIS'\n",
    "processed_dir = data_dir + '/data/processed/PhoATIS'\n",
    "qa_processed = data_dir + '/data/processed/QA'\n",
    "tokenizer = AutoTokenizer.from_pretrained('NlpHUST/bert-base-vn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(off_set_map, text, start, end=False):\n",
    "    '''\n",
    "      if END_IDX is provided, just use it and leave TEXT=''\n",
    "    '''\n",
    "    start_idx, end_idx = [], 0\n",
    "    for i in range(len(off_set_map)):\n",
    "        if off_set_map[i][0] == start:\n",
    "            start_idx.append(i)\n",
    "        elif off_set_map[i][0] == start + 1:\n",
    "            start_idx.append(i)\n",
    "        \n",
    "        if len(text.split()) == 1 and start_idx != 0:\n",
    "            end_idx = start_idx\n",
    "            break\n",
    "\n",
    "        if text != '' and start_idx != 0 and i > start_idx:\n",
    "            if off_set_map[i][1] == start + len(text):\n",
    "              end_idx = i \n",
    "\n",
    "            elif off_set_map[i][1] == start + len(text) + 1:\n",
    "              end_idx = i\n",
    "\n",
    "    # print(\"what\", start_idx, end_idx)\n",
    "    return torch.tensor(start_idx, dtype=torch.long), torch.tensor(end_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??? tensor(499) tensor(14)\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "for i in range(len(dev_df)):\n",
    "    q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "    input = tokenizer(q.strip(), c, return_tensors='pt',\n",
    "                max_length=500,\n",
    "                truncation=\"only_second\",\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\",)\n",
    "    a, t = string2list(a, 'int'), string2list(t)\n",
    "    start, end = 0, 0\n",
    "    for i in range(len(a)):\n",
    "        s, e = get_label(input['offset_mapping'][0], t[0], a[0])\n",
    "        if e < s:\n",
    "            print(\"???\", s, e)\n",
    "            run = False\n",
    "            break\n",
    "        start += s \n",
    "        end += e\n",
    "    # if start == 0 or end == 0:\n",
    "    #     q, c, a, t = q, c, a, t\n",
    "    #     run = False\n",
    "    #     break\n",
    "    if not run:\n",
    "        break\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bản đồ địa chất được bắt đầu vẽ ở đâu?',\n",
       " 'Bồn Paris tạo nên một tập hợp các lớp trầm tích kế tiếp. Đây là một trong những nơi đầu tiên trở thành đối tượng của việc xây dựng bản đồ địa chất và cho phép tạo ra nhiều lý thuyết về địa chất, như cổ sinh vật học và giải phẫu so sánh của Georges Cuvier. Bồn Paris được tạo trong khoảng thời gian 41 ngàn năm. Đây là một bồn thềm lục địa trên những khối núi từ thời Đại cổ sinh như khối núi Vosges, khối núi Massif central, khối núi Armorica. Với sự tạo thành của dãy Alps, bồn Paris bị đóng lại, chỉ còn mở ra phía biển Manche và Đại Tây Dương. Nó báo trước cho sự hình thành lưu vực sông Loire và sông Seine. Cuối thế Oligocen, bồn Paris trở thành lục địa.',\n",
       " 'Bồn Paris',\n",
       " 0,\n",
       " 'Bồn Paris tạo nên một tập hợp các lớp trầm tích kế tiếp. Đây là một trong những nơi đầu tiên trở thành đối tượng của việc xây dựng bản đồ địa chất và cho phép tạo ra nhiều lý thuyết về địa chất, như cổ sinh vật học và giải phẫu so sánh của Georges Cuvier. Bồn Paris được tạo trong khoảng thời gian 41 ngàn năm. Đây là một bồn thềm lục địa trên những khối núi từ thời Đại cổ sinh như khối núi Vosges, khối núi Massif central, khối núi Armorica. Với sự tạo thành của dãy Alps, bồn Paris bị đóng lại, chỉ còn mở ra phía biển Manche và Đại Tây Dương. Nó báo trước cho sự hình thành lưu vực sông Loire và sông Seine. Cuối thế Oligocen, bồn Paris trở thành lục địa.')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, c, t[0], a[0], c[a[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(q.strip(), c, return_tensors='pt',\n",
    "                max_length=386,\n",
    "                truncation=\"only_second\",\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\",)\n",
    "# a, t = string2list(a, 'int'), string2list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'Bồn Paris',\n",
       " 'Bản đồ địa chất được bắt đầu vẽ ở đâu?',\n",
       " 'Bồn Paris tạo nên một tập hợp các lớp trầm tích kế tiếp. Đây là một trong những nơi đầu tiên trở thành đối tượng của việc xây dựng bản đồ địa chất và cho phép tạo ra nhiều lý thuyết về địa chất, như cổ sinh vật học và giải phẫu so sánh của Georges Cuvier. Bồn Paris được tạo trong khoảng thời gian 41 ngàn năm. Đây là một bồn thềm lục địa trên những khối núi từ thời Đại cổ sinh như khối núi Vosges, khối núi Massif central, khối núi Armorica. Với sự tạo thành của dãy Alps, bồn Paris bị đóng lại, chỉ còn mở ra phía biển Manche và Đại Tây Dương. Nó báo trước cho sự hình thành lưu vực sông Loire và sông Seine. Cuối thế Oligocen, bồn Paris trở thành lục địa.')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0], t[0], q, c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['overflow_to_sample_mapping']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = input.sequence_ids()\n",
    "idx = 0\n",
    "while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "context_start = idx\n",
    "while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "context_end = idx - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "    start_positions.append(0)\n",
    "    end_positions.append(0)\n",
    "else:\n",
    "    # Otherwise it's the start and end token positions\n",
    "    idx = context_start\n",
    "    while idx <= context_end and offset[idx][0] <= start_char:\n",
    "        idx += 1\n",
    "    start_positions.append(idx - 1)\n",
    "    idx = context_end\n",
    "    while idx >= context_start and offset[idx][1] >= end_char:\n",
    "        idx -= 1\n",
    "    end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "12\n",
      "13\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(385), tensor(14))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = get_label(input['offset_mapping'][0], t[2], a[2])\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input.input_ids[0][start.item():end.item()+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for i in range(len(dev_df)):\n",
    "    q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "    a, c = string2list(a, 'int'), string2list(c)\n",
    "    length.append(len(q.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.979868708971553"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(length)/len(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([432, 432, 432, 432],\n",
       " ['Năm 1871, Đức trở thành một quốc gia dân tộc khi hầu hết các quốc gia Đức thống nhất trong Đế quốc Đức do Phổ chi phối. Sau Chiến tranh thế giới thứ nhất và Cách mạng Đức 1918-1919, Đế quốc này bị thay thế bằng Cộng hòa Weimar theo chế độ nghị viện. Chế độ độc tài quốc xã được hình thành vào năm 1933, dẫn tới Chiến tranh thế giới thứ hai và một nạn diệt chủng. Sau một giai đoạn Đồng Minh chiếm đóng, hai nước Đức được thành lập: Cộng hòa Liên bang Đức và Cộng hòa Dân chủ Đức. Năm 1990, quốc gia được tái thống nhất.'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for i in range(len(df)):\n",
    "    item = df.iloc[i]\n",
    "    start = string2list(item['start'], type='int')\n",
    "    text = string2list(item['text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wefwefwefwfe', 'dfsdfsdf']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"wefwefwefwfe@@@dfsdfsdf\"\n",
    "a.split(\"@@@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, AutoTokenizer\n",
    "model_checkpoint = 'vinai/phobert-base'\n",
    "# model = QAModule(model_checkpoint=model_checkpoint)\n",
    "model = RobertaModel.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "input = tokenizer(\"DIT CON ME\", return_tensors='pt',\n",
    "        max_length=258,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",)\n",
    "model(input_ids=input.input_ids.repeat(16, 1), attention_mask=input.attention_mask.repeat(16, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xps/educate/code/hust/XQA/src/visualization.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/educate/code/hust/XQA/src/visualization.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mdevice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73fdcbcaa6b22d852c0f9bd9783ab6b1b1c25c52a0a8da76beae07513436cb85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
