{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(32, 8)\n",
    "b = torch.zeros([32, 8], dtype=torch.float)\n",
    "b[0, 0] = 1\n",
    "loss = nn.BCEWithLogitsLoss(reduction='none')(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.where(b != 0, loss, torch.tensor([0.]))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.where(b != 0, loss, loss*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 5  # number of tags is 5\n",
    "model = CRF(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "emissions = torch.randn(seq_length, batch_size, num_tags) #n_seq x batch x logits\n",
    "tags = torch.tensor([\n",
    "  [0, 1], [2, 4], [3, 1]\n",
    "], dtype=torch.long)  #nseq x batch\n",
    "loss = model(emissions, tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(30, 32, 142)\n",
    "b = torch.zeros(30, 32, dtype=torch.long)\n",
    "model = CRF(a.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4717.1206, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "name = ['train', 'test', 'dev']\n",
    "for i in name:\n",
    "    for label in open(\"/home/xps/educate/code/hust/XQA/docs/src/JointIDSF/PhoATIS/word-level\" + f'/{i}/label'):\n",
    "        if label.strip() not in list:\n",
    "            list.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_train.csv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_flight_time', 'atis_airfare', 'atis_aircraft',\n",
       "        'atis_ground_service', 'atis_airline', 'atis_abbreviation',\n",
       "        'atis_quantity'], dtype=object),\n",
       " 4834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'vietanh': 'ngu'}\n",
    "a.update({'vie anh': 'ocho'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['atis_flight', 'atis_airfare', 'atis_ground_service',\n",
       "        'atis_airline', 'atis_flight_time', 'atis_quantity',\n",
       "        'atis_abbreviation', 'atis_aircraft'], dtype=object),\n",
       " 800)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/raw/ATIS/atis_intents_test.csv\", header=None)\n",
    "test[0].unique(), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tôi muốn một chuyến bay khứ_hồi từ đồng_hới đến côn_đảo khởi_hành vào thứ tư ngày 16 tháng 6 đến côn_đảo vào buổi tối khoảng 7 giờ và trở về đồng_hới vào buổi tối ngày hôm sau khoảng 7 giờ hãng hàng_không nào có chuyến bay phù_hợp với tuyến đường đó\n"
     ]
    }
   ],
   "source": [
    "lengths = {}\n",
    "with open(\"/home/xps/educate/code/hust/XQA/data/raw/PhoATIS/word-level/train/seq.in\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line.strip().split(' ')) not in lengths.keys():\n",
    "            lengths[len(line.strip().split(' '))] = 1\n",
    "        else:\n",
    "            lengths[len(line.strip().split(' '))] += 1 \n",
    "        if len(line.strip().split(' ')) == 50:\n",
    "            print(line.strip()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lengths, key = lambda d: lengths[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoctro'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename('/hoa/hoctro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 358/358 [00:00<00:00, 152kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 834k/834k [00:01<00:00, 496kB/s]  \n",
      "Downloading merges.txt: 100%|██████████| 500k/500k [00:01<00:00, 349kB/s]  \n",
      "Downloading tokenizer.json: 100%|██████████| 2.12M/2.12M [00:11<00:00, 195kB/s] \n",
      "Downloading special_tokens_map.json: 100%|██████████| 280/280 [00:00<00:00, 138kB/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "os.environ['dir'] = \"/home/xps/educate/code/hust/XQA/data\"\n",
    "from utils.preprocess import string2list, get_label\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "data_dir = os.environ['dir']\n",
    "raw_dir = data_dir + '/data/raw/PhoATIS'\n",
    "processed_dir = data_dir + '/data/processed/PhoATIS'\n",
    "qa_processed = data_dir + '/data/processed/QA'\n",
    "tokenizer = AutoTokenizer.from_pretrained('NlpHUST/roberta-base-vn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"/home/xps/educate/code/hust/XQA/data/processed/QA/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[449, 450, 450, 449]\n",
      "[449, 450, 450, 449]\n",
      "[469, 470, 471, 470]\n",
      "[469, 470, 471, 470]\n",
      "[469, 470, 471, 470]\n",
      "[144, 143, 143, 143]\n",
      "[144, 143, 143, 143]\n",
      "[144, 143, 143, 143]\n",
      "[577, 470, 471, 577]\n",
      "[115, 116, 115, 115]\n",
      "[115, 116, 115, 115]\n",
      "[115, 116, 115, 115]\n",
      "[412, 413, 413, 413]\n",
      "[67, 68, 68, 67]\n",
      "[67, 68, 68, 67]\n",
      "[491, 491, 490, 491]\n",
      "[474, 474, 475, 474]\n",
      "[474, 474, 475, 474]\n",
      "[474, 474, 475, 474]\n",
      "[304, 297, 298, 297]\n",
      "[304, 297, 298, 297]\n",
      "[285, 286, 286, 285]\n",
      "[285, 286, 286, 285]\n",
      "[464, 463, 463, 463]\n",
      "[464, 463, 463, 463]\n",
      "[464, 463, 463, 463]\n",
      "[44, 43, 43, 43]\n",
      "[44, 43, 43, 43]\n",
      "[44, 43, 43, 43]\n",
      "[109, 104, 108, 109]\n",
      "[441, 440, 440, 440]\n",
      "[441, 440, 440, 440]\n",
      "[441, 440, 440, 440]\n",
      "[524, 523, 524, 523]\n",
      "[524, 523, 524, 523]\n",
      "[193, 194, 194, 194]\n",
      "[92, 93, 93, 93]\n",
      "[189, 190, 190, 189]\n",
      "[189, 190, 190, 189]\n",
      "[368, 369, 369, 369]\n",
      "[882, 879, 883, 882]\n",
      "[882, 879, 883, 882]\n",
      "[50, 51, 50, 50]\n",
      "[50, 51, 50, 50]\n",
      "[50, 51, 50, 50]\n",
      "[455, 456, 455, 455]\n",
      "[455, 456, 455, 455]\n",
      "[455, 456, 455, 455]\n",
      "[613, 613, 614, 613]\n",
      "[613, 613, 614, 613]\n",
      "[613, 613, 614, 613]\n",
      "[106, 105, 105, 105]\n",
      "[106, 105, 105, 105]\n",
      "[106, 105, 105, 105]\n",
      "[419, 420, 420, 420]\n",
      "[51, 118, 52, 51]\n",
      "[51, 118, 52, 51]\n",
      "[340, 341, 347, 340]\n",
      "[340, 341, 347, 340]\n",
      "[197, 198, 198, 198]\n",
      "[265, 266, 266, 266]\n",
      "[422, 428, 427, 428]\n",
      "[699, 700, 700, 700]\n",
      "[691, 692, 692, 692]\n",
      "[531, 541, 532, 541]\n",
      "[833, 834, 834, 834]\n",
      "[723, 724, 724, 724]\n",
      "[336, 366, 337, 336]\n",
      "[336, 366, 337, 336]\n",
      "[654, 655, 655, 655]\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "for i in range(len(dev_df)):\n",
    "    q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "    a, t = string2list(a, 'int'), string2list(t)\n",
    "    for i in range(len(a)):\n",
    "        if a[i] + 1 in a:\n",
    "            print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Nổi tiếng với tên gọi Kinh đô ánh sáng, Paris là một trung tâm văn hóa lớn của thế giới và cũng là một trong những thành phố du lịch thu hút nhất. Sự nhộn nhịp, các công trình kiến trúc và không khí nghệ sĩ đã giúp Paris mỗi năm có đến 30 triệu khách nước ngoài. Thành phố còn được xem như kinh đô của thời trang cao cấp với nhiều khu phố xa xỉ cùng các trung tâm thương mại lớn. Là nơi đặt trụ sở chính của các tổ chức quốc tế như OECD, UNESCO... cộng với những hoạt động đa dạng về tài chính, kinh doanh, chính trị và du lịch đã khiến Paris trở thành một trong những trung tâm trung chuyển lớn nhất trên thế giới và được coi như một trong bốn \"thành phố toàn cầu\" cùng với New York, Luân Đôn và Tokyo.',\n",
       " '@@@mỗi năm có đến 30 triệu khách nước ngoài@@@Kinh đô ánh sáng@@@Kinh đô ánh sáng@@@Kinh đô ánh sáng',\n",
       " '[221, 22, 22, 22]',\n",
       " 'Điều gì đã nói lên Paris là thành phố lý tưởng để khách du lịch?')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "c, t, a, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label(input, text, start):\n",
    "\n",
    "#     sequence_ids = input.sequence_ids()\n",
    "#     idx = 0\n",
    "#     while sequence_ids[idx] != 1:\n",
    "#             idx += 1\n",
    "#     context_start = idx\n",
    "#     while sequence_ids[idx] == 1:\n",
    "#             idx += 1\n",
    "#     context_end = idx - 1\n",
    "#     offset = input['offset_mapping']\n",
    "    \n",
    "\n",
    "#     start_positions, end_positions = [], []\n",
    "#     start_positions = []\n",
    "#     end_positions = []\n",
    "#     start_char = start\n",
    "#     end_char = start + len(text)\n",
    "#     offset = input['offset_mapping'][0]\n",
    "#     if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "#         start_positions.append(0)\n",
    "#         end_positions.append(0)\n",
    "#     else:\n",
    "#         # Otherwise it's the start and end token positions\n",
    "#         idx = context_start\n",
    "#         while idx <= context_end and offset[idx][0] <= start_char:\n",
    "#             idx += 1\n",
    "#         start_positions.append(idx - 1)\n",
    "#         idx = context_end\n",
    "#         while idx >= context_start and offset[idx][1] >= end_char:\n",
    "#             idx -= 1\n",
    "#         end_positions.append(idx + 1)\n",
    "\n",
    " \n",
    "#     return torch.tensor(start_positions, dtype=torch.long), torch.tensor(end_positions, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Đối tượng nào đã ra tay với tờ báo Charlie Hebdo vào năm 2015?',\n",
       " 'Vào ngày 7 tháng năm 2015, hai kẻ Hồi giáo cực đoan tấn công trụ sở của tờ báo Charlie Hebdo và giết chết mười ba người và vào ngày 9 tháng 1, một tên khủng bố thứ ba, tuyên bố y là một thành viên của ISIS, đã giết chết bốn con tin trong cuộc tấn công vào một cửa hàng tạp hóa của người Do Thái ở Porte de Vincennes. Một loạt các cuộc tuần hành diễn ra ở các thành phố trên toàn nước Pháp vào ngày 10 và 11 tháng 1 năm 2015 để tưởng niệm các nạn nhân của vụ tấn công Charlie Hebdo, vụ nổ súng tại Montrouge, và cuộc khủng hoảng con tin Porte de Vincennes, đồng thời lên tiếng ủng hộ cho tự do ngôn luận, tự do báo chí và chống chủ nghĩa khủng bố. Các quan chức Pháp ước tính rằng các cuộc tuần hành đã có sự tham gia của khoảng bốn triệu người dân cả nước, trở thành đợt tuần hành công cộng lớn nhất tại Pháp kể từ năm 1944, khi Paris được giải phóng khỏi tay Đức Quốc xã vào cuối cuộc Chiến tranh thế giới thứ hai.',\n",
       " 'hai kẻ Hồi giáo cực đoan',\n",
       " 27,\n",
       " 'hai kẻ Hồi giáo cực đoan tấn công trụ sở của tờ báo Charlie Hebdo và giết chết mười ba người và vào ngày 9 tháng 1, một tên khủng bố thứ ba, tuyên bố y là một thành viên của ISIS, đã giết chết bốn con tin trong cuộc tấn công vào một cửa hàng tạp hóa của người Do Thái ở Porte de Vincennes. Một loạt các cuộc tuần hành diễn ra ở các thành phố trên toàn nước Pháp vào ngày 10 và 11 tháng 1 năm 2015 để tưởng niệm các nạn nhân của vụ tấn công Charlie Hebdo, vụ nổ súng tại Montrouge, và cuộc khủng hoảng con tin Porte de Vincennes, đồng thời lên tiếng ủng hộ cho tự do ngôn luận, tự do báo chí và chống chủ nghĩa khủng bố. Các quan chức Pháp ước tính rằng các cuộc tuần hành đã có sự tham gia của khoảng bốn triệu người dân cả nước, trở thành đợt tuần hành công cộng lớn nhất tại Pháp kể từ năm 1944, khi Paris được giải phóng khỏi tay Đức Quốc xã vào cuối cuộc Chiến tranh thế giới thứ hai.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 100\n",
    "q, a, c, t = dev_df.iloc[i][['question', 'start', 'context', 'text']]\n",
    "input = tokenizer(q.strip(), c, return_tensors='pt',\n",
    "            max_length=500,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",)\n",
    "a, t = string2list(a, 'int'), string2list(t)\n",
    "q, c, t[0], a[0], c[a[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2idx(start, end, context):\n",
    "    subtext = context[start:end+1]\n",
    "    subtext2 = context[start:end+2]\n",
    "\n",
    "    for i in range(len(context.split())):\n",
    "        if context.split()[i] == subtext.split()[0] and context.split()[i+len(subtext.split())-1] == subtext.split()[-1]:\n",
    "            return i, i+len(subtext.split()) - 1\n",
    "        elif context.split()[i] == subtext2.split()[0] and context.split()[i+len(subtext2.split())-1] == subtext2.split()[-1]:\n",
    "            return i, i+len(subtext2.split()) - 1 \n",
    "def offset2length(offset_map):\n",
    "    word_lengths = []\n",
    "    length = 1\n",
    "    offset_map = [offset_map[i] for i in range(len(offset_map)) if offset_map[i].sum() != 0]\n",
    "    for idx in range(len(offset_map)):\n",
    "        print(offset_map[idx])\n",
    "        \n",
    "\n",
    "        if offset_map[idx][0] == offset_map[idx-1][1]:\n",
    "            length += 1\n",
    "        else:\n",
    "            \n",
    "            word_lengths.append(length)\n",
    "            length = 1\n",
    "    word_lengths.append(length)\n",
    "            \n",
    "    return word_lengths\n",
    "def QA_metrics(start, end, start_idx, end_idx, input, tokenizer):\n",
    "    '''\n",
    "    EM and F1 score for text output\n",
    "    start = b x n\n",
    "    '''\n",
    "    EM = 0\n",
    "    F1 = 0\n",
    "    for i in range(start.shape[0]):\n",
    "        pred = tokenizer.decode(input.input_ids[0][start[i]:end[i]+1])\n",
    "        true = tokenizer.decode(input.input_ids[0][start_idx[i]:end_idx[i]+1])\n",
    "        if pred == true:\n",
    "            EM += 1 \n",
    "        sum = 0\n",
    "        text = pred if len(pred.split()) < len(true.split()) else true\n",
    "        for i in range(len(text.split())):\n",
    "            if pred.split()[i] == true.split()[i]:\n",
    "                sum += 1 \n",
    "        precision = sum/len(pred.split())\n",
    "        recall = sum/len(true.split())\n",
    "        F1 += 2/(1/precision + 1/recall)\n",
    "    return EM/start.shape[0], F1/start.shape[0]\n",
    "    \n",
    "\n",
    "\n",
    "def get_label(input, text, start, reverse=False, max_length=300, context=None, question=None):\n",
    "    '''\n",
    "    we can make use of original sequence or sub_word sequence (tokenized for labelling)\n",
    "    - non-reverse: use the mapping to map start and end idx\n",
    "    - reverse: og start and end idx, and word lengths for mapping, use context for get start and end\n",
    "    '''\n",
    "\n",
    "    if reverse:\n",
    "        start_positions, end_positions = start, start + len(text) - 1\n",
    "        start_positions, end_positions = char2idx(start_positions, end_positions, context)\n",
    "        offset_mapping = input.offset_mapping[0]\n",
    "        word_lengths = offset2length(offset_mapping)\n",
    "        # while len(word_lengths) < max_length:\n",
    "        #     word_lengths.append(1)\n",
    "        return torch.tensor(start_positions, dtype=torch.long), torch.tensor(end_positions, dtype=torch.long), torch.tensor(word_lengths, dtype=torch.long)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    sequence_ids = input.sequence_ids()\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "    context_end = idx - 1\n",
    "    offset = input['offset_mapping']\n",
    "    \n",
    "\n",
    "    start_positions, end_positions = 0, 0\n",
    "    start_char = start\n",
    "    end_char = start + len(text)\n",
    "    offset = input['offset_mapping'][0]\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions = 0\n",
    "        end_positions = 0\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions = idx - 1\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions = idx + 1\n",
    "    \n",
    "\n",
    "\n",
    " \n",
    "    return torch.tensor(start_positions, dtype=torch.long), torch.tensor(end_positions, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3])\n",
      "tensor([4, 8])\n",
      "tensor([ 9, 10])\n",
      "tensor([11, 16])\n",
      "tensor([17, 20])\n",
      "tensor([21, 25])\n",
      "tensor([25, 26])\n",
      "tensor([27, 30])\n",
      "tensor([31, 33])\n",
      "tensor([34, 37])\n",
      "tensor([38, 42])\n",
      "tensor([43, 46])\n",
      "tensor([47, 51])\n",
      "tensor([52, 55])\n",
      "tensor([56, 60])\n",
      "tensor([61, 64])\n",
      "tensor([65, 67])\n",
      "tensor([68, 71])\n",
      "tensor([72, 74])\n",
      "tensor([75, 78])\n",
      "tensor([79, 86])\n",
      "tensor([87, 90])\n",
      "tensor([90, 92])\n",
      "tensor([93, 95])\n",
      "tensor([ 96, 100])\n",
      "tensor([101, 105])\n",
      "tensor([106, 110])\n",
      "tensor([111, 113])\n",
      "tensor([114, 119])\n",
      "tensor([120, 122])\n",
      "tensor([123, 126])\n",
      "tensor([127, 131])\n",
      "tensor([132, 133])\n",
      "tensor([134, 139])\n",
      "tensor([140, 141])\n",
      "tensor([141, 142])\n",
      "tensor([143, 146])\n",
      "tensor([147, 150])\n",
      "tensor([151, 156])\n",
      "tensor([157, 159])\n",
      "tensor([160, 163])\n",
      "tensor([164, 166])\n",
      "tensor([166, 167])\n",
      "tensor([168, 173])\n",
      "tensor([174, 176])\n",
      "tensor([177, 178])\n",
      "tensor([179, 181])\n",
      "tensor([182, 185])\n",
      "tensor([186, 191])\n",
      "tensor([192, 196])\n",
      "tensor([197, 200])\n",
      "tensor([201, 205])\n",
      "tensor([205, 206])\n",
      "tensor([207, 209])\n",
      "tensor([210, 214])\n",
      "tensor([215, 219])\n",
      "tensor([220, 223])\n",
      "tensor([224, 227])\n",
      "tensor([228, 231])\n",
      "tensor([232, 237])\n",
      "tensor([238, 242])\n",
      "tensor([243, 246])\n",
      "tensor([247, 251])\n",
      "tensor([252, 255])\n",
      "tensor([256, 259])\n",
      "tensor([260, 263])\n",
      "tensor([264, 268])\n",
      "tensor([269, 272])\n",
      "tensor([273, 276])\n",
      "tensor([277, 280])\n",
      "tensor([281, 286])\n",
      "tensor([287, 289])\n",
      "tensor([290, 294])\n",
      "tensor([295, 296])\n",
      "tensor([297, 300])\n",
      "tensor([300, 302])\n",
      "tensor([303, 305])\n",
      "tensor([306, 310])\n",
      "tensor([310, 315])\n",
      "tensor([315, 316])\n",
      "tensor([317, 320])\n",
      "tensor([321, 325])\n",
      "tensor([326, 329])\n",
      "tensor([330, 334])\n",
      "tensor([335, 339])\n",
      "tensor([340, 344])\n",
      "tensor([345, 349])\n",
      "tensor([350, 352])\n",
      "tensor([353, 354])\n",
      "tensor([355, 358])\n",
      "tensor([359, 364])\n",
      "tensor([365, 368])\n",
      "tensor([369, 373])\n",
      "tensor([374, 378])\n",
      "tensor([379, 383])\n",
      "tensor([384, 388])\n",
      "tensor([389, 392])\n",
      "tensor([393, 397])\n",
      "tensor([398, 400])\n",
      "tensor([401, 403])\n",
      "tensor([404, 406])\n",
      "tensor([407, 412])\n",
      "tensor([413, 414])\n",
      "tensor([415, 418])\n",
      "tensor([419, 423])\n",
      "tensor([424, 426])\n",
      "tensor([427, 432])\n",
      "tensor([433, 437])\n",
      "tensor([438, 441])\n",
      "tensor([442, 445])\n",
      "tensor([446, 450])\n",
      "tensor([451, 454])\n",
      "tensor([455, 457])\n",
      "tensor([458, 461])\n",
      "tensor([462, 466])\n",
      "tensor([467, 474])\n",
      "tensor([475, 478])\n",
      "tensor([478, 480])\n",
      "tensor([480, 481])\n",
      "tensor([482, 484])\n",
      "tensor([485, 487])\n",
      "tensor([488, 492])\n",
      "tensor([493, 496])\n",
      "tensor([497, 501])\n",
      "tensor([501, 504])\n",
      "tensor([504, 506])\n",
      "tensor([506, 507])\n",
      "tensor([508, 510])\n",
      "tensor([511, 515])\n",
      "tensor([516, 521])\n",
      "tensor([522, 527])\n",
      "tensor([528, 531])\n",
      "tensor([532, 535])\n",
      "tensor([536, 539])\n",
      "tensor([539, 541])\n",
      "tensor([542, 544])\n",
      "tensor([545, 549])\n",
      "tensor([549, 554])\n",
      "tensor([554, 555])\n",
      "tensor([556, 560])\n",
      "tensor([561, 565])\n",
      "tensor([566, 569])\n",
      "tensor([570, 575])\n",
      "tensor([576, 579])\n",
      "tensor([580, 582])\n",
      "tensor([583, 586])\n",
      "tensor([587, 589])\n",
      "tensor([590, 592])\n",
      "tensor([593, 597])\n",
      "tensor([598, 602])\n",
      "tensor([602, 603])\n",
      "tensor([604, 606])\n",
      "tensor([607, 609])\n",
      "tensor([610, 613])\n",
      "tensor([614, 617])\n",
      "tensor([618, 620])\n",
      "tensor([621, 626])\n",
      "tensor([627, 630])\n",
      "tensor([631, 636])\n",
      "tensor([637, 642])\n",
      "tensor([643, 645])\n",
      "tensor([645, 646])\n",
      "tensor([647, 650])\n",
      "tensor([651, 655])\n",
      "tensor([656, 660])\n",
      "tensor([661, 665])\n",
      "tensor([666, 669])\n",
      "tensor([670, 674])\n",
      "tensor([675, 679])\n",
      "tensor([680, 683])\n",
      "tensor([684, 688])\n",
      "tensor([689, 693])\n",
      "tensor([694, 698])\n",
      "tensor([699, 701])\n",
      "tensor([702, 704])\n",
      "tensor([705, 707])\n",
      "tensor([708, 712])\n",
      "tensor([713, 716])\n",
      "tensor([717, 720])\n",
      "tensor([721, 727])\n",
      "tensor([728, 731])\n",
      "tensor([732, 737])\n",
      "tensor([738, 743])\n",
      "tensor([744, 747])\n",
      "tensor([748, 750])\n",
      "tensor([751, 755])\n",
      "tensor([755, 756])\n",
      "tensor([757, 760])\n",
      "tensor([761, 766])\n",
      "tensor([767, 770])\n",
      "tensor([771, 775])\n",
      "tensor([776, 780])\n",
      "tensor([781, 785])\n",
      "tensor([786, 790])\n",
      "tensor([791, 794])\n",
      "tensor([795, 799])\n",
      "tensor([800, 803])\n",
      "tensor([804, 808])\n",
      "tensor([809, 811])\n",
      "tensor([812, 814])\n",
      "tensor([815, 818])\n",
      "tensor([819, 823])\n",
      "tensor([823, 824])\n",
      "tensor([825, 828])\n",
      "tensor([829, 834])\n",
      "tensor([835, 839])\n",
      "tensor([840, 844])\n",
      "tensor([845, 850])\n",
      "tensor([851, 855])\n",
      "tensor([856, 859])\n",
      "tensor([860, 863])\n",
      "tensor([864, 868])\n",
      "tensor([869, 871])\n",
      "tensor([872, 875])\n",
      "tensor([876, 880])\n",
      "tensor([881, 885])\n",
      "tensor([886, 891])\n",
      "tensor([892, 897])\n",
      "tensor([898, 901])\n",
      "tensor([902, 906])\n",
      "tensor([907, 910])\n",
      "tensor([911, 914])\n",
      "tensor([914, 915])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Vào ngày 7 tháng năm 2015, hai kẻ Hồi giáo cực đoan tấn công trụ sở của tờ báo Charlie Hebdo và giết chết mười ba người và vào ngày 9 tháng 1, một tên khủng bố thứ ba, tuyên bố y là một thành viên của ISIS, đã giết chết bốn con tin trong cuộc tấn công vào một cửa hàng tạp hóa của người Do Thái ở Porte de Vincennes. Một loạt các cuộc tuần hành diễn ra ở các thành phố trên toàn nước Pháp vào ngày 10 và 11 tháng 1 năm 2015 để tưởng niệm các nạn nhân của vụ tấn công Charlie Hebdo, vụ nổ súng tại Montrouge, và cuộc khủng hoảng con tin Porte de Vincennes, đồng thời lên tiếng ủng hộ cho tự do ngôn luận, tự do báo chí và chống chủ nghĩa khủng bố. Các quan chức Pháp ước tính rằng các cuộc tuần hành đã có sự tham gia của khoảng bốn triệu người dân cả nước, trở thành đợt tuần hành công cộng lớn nhất tại Pháp kể từ năm 1944, khi Paris được giải phóng khỏi tay Đức Quốc xã vào cuối cuộc Chiến tranh thế giới thứ hai.'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char2idx(a[0], a[0]+len(t[0])-1, c), c[a[0]:a[0]+len(t[0])]\n",
    "# s, e, w = get_label(input, t[0], a[0], reverse=True, context=c, question=q)\n",
    "# len(w), len((q + \" \" + c).split())\n",
    "input = tokenizer(c, return_offsets_mapping=True, return_tensors='pt')\n",
    "w = offset2length(input.offset_mapping[0])\n",
    "text = c\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vào',\n",
       " 'ngày',\n",
       " '7',\n",
       " 'tháng',\n",
       " 'năm',\n",
       " '2015',\n",
       " ',hai',\n",
       " 'kẻ',\n",
       " 'Hồi',\n",
       " 'giáo',\n",
       " 'cực',\n",
       " 'đoan',\n",
       " 'tấn',\n",
       " 'công',\n",
       " 'trụ',\n",
       " 'sở',\n",
       " 'của',\n",
       " 'tờ',\n",
       " 'báo',\n",
       " 'Charlie',\n",
       " 'Heb',\n",
       " 'dovà',\n",
       " 'giết',\n",
       " 'chết',\n",
       " 'mười',\n",
       " 'ba',\n",
       " 'người',\n",
       " 'và',\n",
       " 'vào',\n",
       " 'ngày',\n",
       " '9',\n",
       " 'tháng',\n",
       " '1',\n",
       " ',một',\n",
       " 'tên',\n",
       " 'khủng',\n",
       " 'bố',\n",
       " 'thứ',\n",
       " 'ba',\n",
       " ',tuyên',\n",
       " 'bố',\n",
       " 'y',\n",
       " 'là',\n",
       " 'một',\n",
       " 'thành',\n",
       " 'viên',\n",
       " 'của',\n",
       " 'ISIS',\n",
       " ',đã',\n",
       " 'giết',\n",
       " 'chết',\n",
       " 'bốn',\n",
       " 'con',\n",
       " 'tin',\n",
       " 'trong',\n",
       " 'cuộc',\n",
       " 'tấn',\n",
       " 'công',\n",
       " 'vào',\n",
       " 'một',\n",
       " 'cửa',\n",
       " 'hàng',\n",
       " 'tạp',\n",
       " 'hóa',\n",
       " 'của',\n",
       " 'người',\n",
       " 'Do',\n",
       " 'Thái',\n",
       " 'ở',\n",
       " 'Por',\n",
       " 'tede',\n",
       " 'Vinc',\n",
       " 'ennes.Một',\n",
       " 'loạt',\n",
       " 'các',\n",
       " 'cuộc',\n",
       " 'tuần',\n",
       " 'hành',\n",
       " 'diễn',\n",
       " 'ra',\n",
       " 'ở',\n",
       " 'các',\n",
       " 'thành',\n",
       " 'phố',\n",
       " 'trên',\n",
       " 'toàn',\n",
       " 'nước',\n",
       " 'Pháp',\n",
       " 'vào',\n",
       " 'ngày',\n",
       " '10',\n",
       " 'và',\n",
       " '11',\n",
       " 'tháng',\n",
       " '1',\n",
       " 'năm',\n",
       " '2015',\n",
       " 'để',\n",
       " 'tưởng',\n",
       " 'niệm',\n",
       " 'các',\n",
       " 'nạn',\n",
       " 'nhân',\n",
       " 'của',\n",
       " 'vụ',\n",
       " 'tấn',\n",
       " 'công',\n",
       " 'Charlie',\n",
       " 'Heb',\n",
       " 'do,vụ',\n",
       " 'nổ',\n",
       " 'súng',\n",
       " 'tại',\n",
       " 'Mont',\n",
       " 'rouge,và',\n",
       " 'cuộc',\n",
       " 'khủng',\n",
       " 'hoảng',\n",
       " 'con',\n",
       " 'tin',\n",
       " 'Por',\n",
       " 'tede',\n",
       " 'Vinc',\n",
       " 'ennes,đồng',\n",
       " 'thời',\n",
       " 'lên',\n",
       " 'tiếng',\n",
       " 'ủng',\n",
       " 'hộ',\n",
       " 'cho',\n",
       " 'tự',\n",
       " 'do',\n",
       " 'ngôn',\n",
       " 'luận',\n",
       " ',tự',\n",
       " 'do',\n",
       " 'báo',\n",
       " 'chí',\n",
       " 'và',\n",
       " 'chống',\n",
       " 'chủ',\n",
       " 'nghĩa',\n",
       " 'khủng',\n",
       " 'bố',\n",
       " '.Các',\n",
       " 'quan',\n",
       " 'chức',\n",
       " 'Pháp',\n",
       " 'ước',\n",
       " 'tính',\n",
       " 'rằng',\n",
       " 'các',\n",
       " 'cuộc',\n",
       " 'tuần',\n",
       " 'hành',\n",
       " 'đã',\n",
       " 'có',\n",
       " 'sự',\n",
       " 'tham',\n",
       " 'gia',\n",
       " 'của',\n",
       " 'khoảng',\n",
       " 'bốn',\n",
       " 'triệu',\n",
       " 'người',\n",
       " 'dân',\n",
       " 'cả',\n",
       " 'nước',\n",
       " ',trở',\n",
       " 'thành',\n",
       " 'đợt',\n",
       " 'tuần',\n",
       " 'hành',\n",
       " 'công',\n",
       " 'cộng',\n",
       " 'lớn',\n",
       " 'nhất',\n",
       " 'tại',\n",
       " 'Pháp',\n",
       " 'kể',\n",
       " 'từ',\n",
       " 'năm',\n",
       " '1944',\n",
       " ',khi',\n",
       " 'Paris',\n",
       " 'được',\n",
       " 'giải',\n",
       " 'phóng',\n",
       " 'khỏi',\n",
       " 'tay',\n",
       " 'Đức',\n",
       " 'Quốc',\n",
       " 'xã',\n",
       " 'vào',\n",
       " 'cuối',\n",
       " 'cuộc',\n",
       " 'Chiến',\n",
       " 'tranh',\n",
       " 'thế',\n",
       " 'giới',\n",
       " 'thứ',\n",
       " 'hai']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "for i in range(len(w)):\n",
    "    word = \"\"\n",
    "    count = 0\n",
    "    length = w[i]\n",
    "    while count < length:\n",
    "        if input.offset_mapping[0][idx] != (0, 0):\n",
    "            s, e = input.offset_mapping[0][idx]\n",
    "            word += text[s:e]\n",
    "            count += 1 \n",
    "        idx += 1\n",
    "    words.append(word)\n",
    "words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73fdcbcaa6b22d852c0f9bd9783ab6b1b1c25c52a0a8da76beae07513436cb85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
